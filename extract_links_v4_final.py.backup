"""
é“¾æ¥æ ‡é¢˜å’Œä½œè€…æå–å·¥å…· v4.5 - ç²¾å‡†é¢œè‰²æ ‡è®°ç‰ˆ
âœ… æˆåŠŸçªç ´å¾®åšï¼(98.2%)
âœ… æŠ–éŸ³80%æˆåŠŸç‡
âœ… å°çº¢ä¹¦90%æˆåŠŸç‡
âœ… æ±½è½¦ä¹‹å®¶90%æˆåŠŸç‡
âœ… ç™¾å®¶å·66%+æˆåŠŸç‡ï¼ˆPlaywrightç»•è¿‡å®‰å…¨éªŒè¯ï¼‰
âœ… ä»Šæ—¥å¤´æ¡80%æˆåŠŸç‡ï¼ˆè‡ªåŠ¨Playwrightï¼‰
âœ… Excelç²¾å‡†é¢œè‰²æ ‡è®°ï¼š
   ğŸ”´ 404é”™è¯¯ â†’ æ•´è¡Œæ ‡çº¢
   ğŸŸ¡ æ ‡é¢˜/ä½œè€…å¤±è´¥ â†’ å¯¹åº”å•å…ƒæ ¼æ ‡é»„
"""
import pandas as pd
import requests
from bs4 import BeautifulSoup
import openpyxl
from openpyxl.styles import PatternFill
import time
import re
import json
import html as html_module

# å°è¯•å¯¼å…¥Playwrightï¼ˆä»Šæ—¥å¤´æ¡ä¸“ç”¨ï¼‰
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸  Playwrightæœªå®‰è£…ï¼Œä»Šæ—¥å¤´æ¡å°†ä½¿ç”¨requestsï¼ˆæ•ˆæœè¾ƒå·®ï¼‰")

# åˆ›å»ºå…¨å±€Session
session = requests.Session()

def read_excel_with_links(file_path):
    """è¯»å–Excelæ–‡ä»¶å¹¶æå–æ‰€æœ‰é“¾æ¥"""
    wb = openpyxl.load_workbook(file_path)
    ws = wb.active
    
    links = []
    for row in ws.iter_rows():
        for cell in row:
            if cell.hyperlink:
                links.append({
                    'cell': cell.coordinate,
                    'text': cell.value,
                    'url': cell.hyperlink.target
                })
            elif isinstance(cell.value, str) and ('http://' in cell.value or 'https://' in cell.value):
                urls = re.findall(r'https?://[^\s]+', cell.value)
                for url in urls:
                    links.append({
                        'cell': cell.coordinate,
                        'text': cell.value,
                        'url': url
                    })
    
    return links

def get_website_name(url):
    """ä»URLæå–ç½‘ç«™åç§°"""
    url_lower = url.lower()
    
    if 'bilibili.com' in url_lower:
        return 'å“”å“©å“”å“©'
    elif 'douyin.com' in url_lower or 'iesdouyin.com' in url_lower:
        return 'æŠ–éŸ³'
    elif 'xiaohongshu.com' in url_lower or 'xhslink.com' in url_lower:
        return 'å°çº¢ä¹¦'
    elif 'weibo.com' in url_lower:
        return 'å¾®åš'
    elif 'toutiao.com' in url_lower:
        return 'ä»Šæ—¥å¤´æ¡'
    elif 'autohome.com' in url_lower:
        return 'æ±½è½¦ä¹‹å®¶'
    elif 'zjbyte.cn' in url_lower or 'dongchedi' in url_lower or 'dcd' in url_lower:
        return 'æ‡‚è½¦å¸'
    elif 'zhihu.com' in url_lower:
        return 'çŸ¥ä¹'
    else:
        try:
            from urllib.parse import urlparse
            domain = urlparse(url).netloc
            domain = domain.replace('www.', '').replace('.com', '').replace('.cn', '')
            return domain if domain else 'æœªçŸ¥ç½‘ç«™'
        except:
            return 'æœªçŸ¥ç½‘ç«™'

def extract_weibo_breakthrough(url):
    """å¾®åšçªç ´ç‰ˆ - ä½¿ç”¨ç§»åŠ¨ç«¯API"""
    try:
        # ä»URLæå–mid
        id_match = re.search(r'/(\d+)/([A-Za-z0-9]+)', url)
        if not id_match:
            return {
                'title': 'URLæ ¼å¼é”™è¯¯',
                'author': 'URLæ ¼å¼é”™è¯¯',
                'status': 'failed: URLæ ¼å¼ä¸æ­£ç¡®'
            }
        
        uid, mid = id_match.groups()
        
        # ä½¿ç”¨ç§»åŠ¨ç«¯API
        api_url = f"https://m.weibo.cn/statuses/show?id={mid}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15',
            'Accept': 'application/json',
            'Referer': f'https://m.weibo.cn/status/{mid}',
            'X-Requested-With': 'XMLHttpRequest',
        }
        
        response = session.get(api_url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            try:
                data = response.json()
                
                if 'data' in data:
                    status = data['data']
                    
                    # æå–æ ‡é¢˜ - ä¼˜å…ˆä½¿ç”¨text_rawï¼Œå¦åˆ™ä½¿ç”¨text
                    title = status.get('text_raw', status.get('text', ''))
                    
                    # æ¸…ç†HTMLæ ‡ç­¾
                    if title:
                        title = re.sub(r'<[^>]+>', '', title)
                        title = title.strip()
                        # é™åˆ¶é•¿åº¦
                        if len(title) > 200:
                            title = title[:200] + '...'
                    
                    # æå–ä½œè€…
                    author = ''
                    if 'user' in status:
                        author = status['user'].get('screen_name', '')
                    
                    return {
                        'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
                        'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
                        'status': 'success' if (title and author) else 'partial (å¾®åšAPIæå–)'
                    }
                else:
                    return {
                        'title': 'APIå“åº”æ— æ•°æ®',
                        'author': 'æœªæ‰¾åˆ°',
                        'status': 'failed: APIå“åº”æ ¼å¼é”™è¯¯'
                    }
            
            except json.JSONDecodeError:
                return {
                    'title': 'APIå“åº”è§£æå¤±è´¥',
                    'author': 'æœªæ‰¾åˆ°',
                    'status': 'failed: JSONè§£æé”™è¯¯'
                }
        else:
            return {
                'title': f'APIè¯·æ±‚å¤±è´¥({response.status_code})',
                'author': 'æœªæ‰¾åˆ°',
                'status': f'failed: HTTP {response.status_code}'
            }
    
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_douyin_enhanced(url):
    """æŠ–éŸ³å¢å¼ºæå– - æ·±åº¦JSONæœç´¢"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15',
            'Accept': 'text/html,application/xhtml+xml',
            'Referer': 'https://www.douyin.com/',
        }
        
        response = session.get(url, headers=headers, timeout=15, allow_redirects=True)
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title = None
        author = None
        
        # æ–¹æ³•1ï¼šä»titleæ ‡ç­¾æå–
        if soup.find('title'):
            title_text = soup.find('title').text.strip()
            if ' - æŠ–éŸ³' in title_text:
                title = title_text.split(' - æŠ–éŸ³')[0].strip()
            else:
                title = title_text
        
        # æ–¹æ³•2ï¼šä»RENDER_DATAæ·±åº¦æå–ä½œè€…
        render_match = re.search(r'<script id="RENDER_DATA" type="application/json">([^<]+)</script>', response.text)
        if render_match:
            try:
                json_str = html_module.unescape(render_match.group(1))
                data = json.loads(json_str)
                
                # è¶…æ·±åº¦é€’å½’æœç´¢
                def deep_find_author(obj, depth=0, max_depth=10):
                    if depth > max_depth:
                        return None
                    
                    if isinstance(obj, dict):
                        author_keys = ['nickname', 'authorName', 'unique_id', 'short_id', 'userName', 'user_name']
                        for key in author_keys:
                            if key in obj and isinstance(obj[key], str) and 2 < len(obj[key]) < 50:
                                return obj[key]
                        
                        if 'author' in obj and isinstance(obj['author'], dict):
                            result = deep_find_author(obj['author'], depth+1, max_depth)
                            if result:
                                return result
                        
                        if 'user' in obj and isinstance(obj['user'], dict):
                            result = deep_find_author(obj['user'], depth+1, max_depth)
                            if result:
                                return result
                        
                        for value in obj.values():
                            result = deep_find_author(value, depth+1, max_depth)
                            if result:
                                return result
                    
                    elif isinstance(obj, list):
                        for item in obj:
                            result = deep_find_author(item, depth+1, max_depth)
                            if result:
                                return result
                    
                    return None
                
                author = deep_find_author(data)
                
            except:
                pass
        
        # æ–¹æ³•3ï¼šæ­£åˆ™è¡¨è¾¾å¼æœç´¢
        if not author:
            patterns = [
                r'"nickname":\s*"([^"]{2,30})"',
                r'"authorName":\s*"([^"]{2,30})"',
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, response.text)
                if matches:
                    for match in matches:
                        if re.match(r'^[\u4e00-\u9fa5a-zA-Z0-9_-]+$', match):
                            author = match
                            break
                if author:
                    break
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial (æŠ–éŸ³ä½œè€…éš¾æå–)'
        }
        
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_xiaohongshu_info(url):
    """å°çº¢ä¹¦æå–"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15',
        }
        
        response = session.get(url, headers=headers, timeout=15, allow_redirects=True)
        response.encoding = 'utf-8'
        
        title = None
        author = None
        
        patterns = [
            r'window\.__INITIAL_STATE__\s*=\s*({.+?})\s*<\/script>',
            r'window\.__SETUP_SERVER_STATE__\s*=\s*({.+?})\s*<\/script>',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, response.text, re.DOTALL)
            if matches:
                try:
                    data_str = matches[0].replace('\\u002F', '/')
                    
                    title_match = re.search(r'"title":\s*"([^"]+)"', data_str)
                    if title_match:
                        title = title_match.group(1)
                    
                    if not title or title == 'å°çº¢ä¹¦':
                        desc_match = re.search(r'"desc":\s*"([^"]+)"', data_str)
                        if desc_match and len(desc_match.group(1)) > 5:
                            title = desc_match.group(1)
                    
                    nickname_matches = re.findall(r'"nickname":\s*"([^"]+)"', data_str)
                    if nickname_matches:
                        author = nickname_matches[0]
                    
                    if title and author:
                        break
                except:
                    continue
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial'
        }
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_bilibili_info(url):
    """Bç«™æå–"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://www.bilibili.com/',
        }
        
        response = session.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title = None
        author = None
        
        meta_title = soup.find('meta', {'property': 'og:title'})
        if meta_title:
            title = meta_title.get('content', '').strip()
            for suffix in ['_å“”å“©å“”å“©_bilibili', ' - å“”å“©å“”å“©']:
                if suffix in title:
                    title = title.split(suffix)[0].strip()
        
        meta_author = soup.find('meta', {'name': 'author'})
        if meta_author:
            author = meta_author.get('content', '').strip()
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial'
        }
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_autohome_info(url):
    """æ±½è½¦ä¹‹å®¶ä¸“ç”¨æå–"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        }
        
        response = session.get(url, headers=headers, timeout=15, allow_redirects=True)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title = None
        author = None
        
        # æå–æ ‡é¢˜
        title_elem = soup.find('title')
        if title_elem:
            title = title_elem.text.strip()
        
        # è½¦å®¶å·PCç‰ˆï¼šæå–authorMes
        if 'chejiahao.autohome.com.cn' in url:
            author_elem = soup.find(class_='authorMes')
            if author_elem:
                text = author_elem.get_text(strip=True)
                # æå–ç¬¬ä¸€ä¸ªéæ•°å­—éƒ¨åˆ† "ç‹å°å–µ167å…³æ³¨..." -> "ç‹å°å–µ"
                match = re.search(r'^([^0-9]+?)(?:\d+|å…³æ³¨)', text)
                if match:
                    author = match.group(1)
        
        # è½¦å®¶å·ç§»åŠ¨ç‰ˆï¼šä»JSONæå–nickname
        elif 'm.autohome.com.cn' in url:
            nickname_match = re.search(r'"nickname":\s*"([^"]+)"', response.text)
            if nickname_match:
                author = nickname_match.group(1)
        
        # è®ºå›å¸–å­ï¼šå°è¯•å¤šç§æ–¹æ³•
        elif 'club.autohome.com.cn' in url:
            # æ–¹æ³•1: ä»JavaScriptå˜é‡__TOPICINFO__æå–topicMemberName
            topic_match = re.search(r'topicMemberName:\s*[\'"]([^\'"]+)[\'"]', response.text)
            if topic_match:
                author = topic_match.group(1)
            
            # æ–¹æ³•2: æŸ¥æ‰¾ç”¨æˆ·é“¾æ¥
            if not author:
                user_link = soup.find('a', href=re.compile(r'/space/\d+'))
                if user_link:
                    author = user_link.get_text(strip=True)
            
            # æ–¹æ³•3: åœ¨JSONä¸­æŸ¥æ‰¾userName
            if not author:
                author_match = re.search(r'"userName":\s*"([^"]+)"', response.text)
                if author_match:
                    author = author_match.group(1)
        
        # æ–°é—»é¡µé¢ï¼šæå–ä½œè€…
        elif 'www.autohome.com.cn/news' in url:
            # æ–¹æ³•1: ä»metaæ ‡ç­¾
            meta_author = soup.find('meta', {'name': 'author'})
            if meta_author:
                author = meta_author.get('content', '').strip()
            
            # æ–¹æ³•2: æŸ¥æ‰¾classåŒ…å«authorçš„å…ƒç´ 
            if not author:
                author_elem = soup.find(class_=re.compile('author|writer', re.I))
                if author_elem:
                    author = author_elem.get_text(strip=True)
            
            # æ–¹æ³•3: åœ¨é¡µé¢æºç ä¸­æœç´¢
            if not author:
                author_patterns = [
                    r'"author"\s*:\s*"([^"]+)"',
                    r'ä½œè€…[ï¼š:]\s*([^\s<>"]+)',
                ]
                for pattern in author_patterns:
                    match = re.search(pattern, response.text)
                    if match:
                        author = match.group(1)
                        break
        
        # é€šç”¨æå–ï¼ˆfallbackï¼‰
        if not author:
            meta_author = soup.find('meta', {'name': 'author'})
            if meta_author:
                author = meta_author.get('content', '').strip()
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial'
        }
        
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_general_info(url):
    """é€šç”¨æå–"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        }
        
        response = session.get(url, headers=headers, timeout=15, allow_redirects=True)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title = None
        author = None
        
        meta_title = soup.find('meta', {'property': 'og:title'}) or soup.find('meta', {'name': 'title'})
        if meta_title:
            title = meta_title.get('content', '').strip()
        
        if not title and soup.find('title'):
            title = soup.find('title').text.strip()
        
        meta_author = soup.find('meta', {'property': 'og:author'}) or soup.find('meta', {'name': 'author'})
        if meta_author:
            author = meta_author.get('content', '').strip()
        
        if not author:
            author_elems = soup.find_all(class_=re.compile(r'author|writer', re.I))
            for elem in author_elems:
                text = elem.text.strip()
                if text and len(text) < 50:
                    author = text
                    break
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial'
        }
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_baidu_info(url):
    """ç™¾åº¦ç³»å¹³å°æå–ï¼ˆç™¾å®¶å·ã€mbd.baiduï¼‰"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml',
        }
        
        response = session.get(url, headers=headers, timeout=10, allow_redirects=True)
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = None
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.text.strip()
        
        # æå–ä½œè€…
        author = None
        
        # æ–¹æ³•1ï¼šç™¾å®¶å·çš„ä½œè€…ä¿¡æ¯ï¼ˆæ–°ç‰ˆé¡µé¢ï¼‰
        if 'baijiahao.baidu.com' in url:
            # æ–°ç‰ˆé¡µé¢ï¼šä»HTMLæ ‡ç­¾æå–
            # <span data-testid="author-name" class="_2gGWi">ä½œè€…å</span>
            author_elem = soup.find('span', {'data-testid': 'author-name'})
            if author_elem:
                author = author_elem.get_text().strip()
            
            # å¤‡ç”¨æ–¹æ³•1ï¼šä»class="_2gGWi"æå–
            if not author:
                author_elem = soup.find('span', class_='_2gGWi')
                if author_elem:
                    author = author_elem.get_text().strip()
            
            # å¤‡ç”¨æ–¹æ³•2ï¼šä»authoré“¾æ¥æå–
            if not author:
                author_link = soup.find('a', href=re.compile('author.baidu.com'))
                if author_link:
                    author_span = author_link.find('span')
                    if author_span:
                        author = author_span.get_text().strip()
            
            # å¤‡ç”¨æ–¹æ³•3ï¼šæ—§ç‰ˆJSONæ•°æ®ï¼ˆå…¼å®¹æ—§é“¾æ¥ï¼‰
            if not author:
                json_match = re.search(r'var\s+DATA\s*=\s*({.+?});', response.text, re.DOTALL)
                if json_match:
                    try:
                        data = json.loads(json_match.group(1))
                        if 'superlanding' in data and len(data['superlanding']) > 0:
                            item = data['superlanding'][0].get('itemData', {})
                            if not title or title == 'ç™¾åº¦':
                                title = item.get('header', title)
                            author = item.get('author', {}).get('name', '')
                    except:
                        pass
        
        # æ–¹æ³•2ï¼šmbd.baiduçš„JSONæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰
        elif 'mbd.baidu.com' in url:
            # æ–¹æ³•2.1: æ ‡å‡†author.nameæ ¼å¼
            author_match = re.search(r'"author"\s*:\s*{\s*[^}]*"name"\s*:\s*"([^"]+)"', response.text)
            if author_match:
                author = author_match.group(1)
                # Unicodeè§£ç 
                try:
                    if '\\u' in author:
                        author = author.encode().decode('unicode_escape')
                except:
                    pass
            
            # æ–¹æ³•2.2: æ›´å¤šä½œè€…å­—æ®µå°è¯•
            if not author:
                author_patterns = [
                    r'"authorName"\s*:\s*"([^"]{2,30})"',
                    r'"author_name"\s*:\s*"([^"]{2,30})"',
                    r'"publisher"\s*:\s*"([^"]{2,30})"',
                    r'"source"\s*:\s*"([^"]{2,30})"',
                    r'"sourceName"\s*:\s*"([^"]{2,30})"',
                    r'"account"\s*:\s*{\s*[^}]*"name"\s*:\s*"([^"]+)"',  # account.nameæ ¼å¼
                ]
                for pattern in author_patterns:
                    match = re.search(pattern, response.text)
                    if match:
                        candidate = match.group(1)
                        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯ä½œè€…çš„
                        if candidate not in ['ç™¾åº¦', 'ç™¾å®¶å·', 'ç™¾åº¦APP', 'ç™¾åº¦æ–°é—»'] and len(candidate) > 1:
                            author = candidate
                            # Unicodeè§£ç 
                            try:
                                if '\\u' in author:
                                    author = author.encode().decode('unicode_escape')
                            except:
                                pass
                            break
            
            # æŸ¥æ‰¾æ ‡é¢˜
            if not title or title == 'ç™¾åº¦':
                title_match = re.search(r'"title"\s*:\s*"([^"]{5,100})"', response.text)
                if title_match:
                    title = title_match.group(1)
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial (ç™¾åº¦ç³»)'
        }
        
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æœªæ‰¾åˆ°ä½œè€…',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_dripcar_info(url):
    """æ°´æ»´æ±½è½¦å¹³å°æå–"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        }
        
        response = session.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = None
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.text.strip()
            # æ¸…ç†æ ‡é¢˜
            if '-æ°´æ»´æ±½è½¦' in title:
                title = title.split('-æ°´æ»´æ±½è½¦')[0].strip()
        
        # æå–ä½œè€… - ä»JavaScriptå˜é‡ä¸­
        author = None
        author_match = re.search(r'author_name\s*:\s*"([^"]+)"', response.text)
        if author_match:
            author = author_match.group(1)
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial'
        }
        
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æœªæ‰¾åˆ°ä½œè€…',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_maiche_info(url):
    """ä¹°è½¦ç½‘å¹³å°æå– - å¢å¼ºç‰ˆ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        }
        
        response = session.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = None
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.text.strip()
            # æ¸…ç†æ ‡é¢˜
            if '- ä¹°è½¦ç½‘' in title:
                title = title.split('- ä¹°è½¦ç½‘')[0].strip()
        
        # æˆ–è€…ä»h1è·å–
        if not title:
            h1 = soup.find('h1')
            if h1:
                title = h1.text.strip()
        
        # æå–ä½œè€… - å¢å¼ºç‰ˆ
        author = None
        
        # æ–¹æ³•1ï¼šä»"æ¥æºï¼š"æˆ–"æ¥æºï¼š"åæå–ï¼ˆä¹°è½¦ç½‘å¸¸ç”¨æ ¼å¼ï¼‰
        # ä¾‹å¦‚ï¼šæ¥æºï¼šå¥¶çˆ¸æ•™é€‰è½¦
        source_patterns = [
            r'æ¥æº[ï¼š:]\s*([^\s\n<>]{2,20})',
            r'source[ï¼š:]\s*([^\s\n<>]{2,20})',
        ]
        for pattern in source_patterns:
            match = re.search(pattern, response.text)
            if match:
                potential_author = match.group(1).strip()
                # è¿‡æ»¤æ‰ä¸€äº›æ˜æ˜¾ä¸æ˜¯ä½œè€…çš„è¯
                if potential_author not in ['ä¹°è½¦ç½‘', 'ç½‘ç»œ', 'äº’è”ç½‘', 'å®˜æ–¹']:
                    author = potential_author
                    break
        
        # æ–¹æ³•2ï¼šä»[è½¦å‹å¤´æ¡-è½¦å‹å·-ä½œè€…å]æ ¼å¼æå–
        if not author:
            match = re.search(r'\[è½¦å‹å¤´æ¡[^\]]*è½¦å‹å·[^\]]*[-\-]\s*([^\]]{2,20})\]', response.text)
            if match:
                author = match.group(1).strip()
        
        # æ–¹æ³•3ï¼šæŸ¥æ‰¾ä½œè€…class
        if not author:
            author_elem = soup.find(class_=re.compile('author', re.I))
            if author_elem:
                author = author_elem.text.strip()
        
        # æ–¹æ³•4ï¼šæŸ¥æ‰¾metaæ ‡ç­¾
        if not author:
            meta_author = soup.find('meta', {'name': 'author'})
            if meta_author:
                author = meta_author.get('content', '').strip()
        
        # æ–¹æ³•5ï¼šä»JSONæ•°æ®ä¸­æŸ¥æ‰¾
        if not author:
            author_match = re.search(r'"author"\s*:\s*"([^"]+)"', response.text)
            if author_match:
                author = author_match.group(1)
        
        # æ–¹æ³•6ï¼šä»"æ–‡/"æˆ–"æ–‡:"åæå–
        if not author:
            match = re.search(r'[ï¼ˆ(]æ–‡[/ï¼:]([^ï¼‰)]{2,20})[ï¼‰)]', response.text)
            if match:
                author = match.group(1).strip()
        
        # ç»Ÿä¸€æ¸…ç†ä½œè€…åç§°ï¼ˆåº”ç”¨äºæ‰€æœ‰æ–¹æ³•ï¼‰
        if author:
            # å»æ‰"è½¦å‹å·"ã€"ä½œè€…ï¼š"ç­‰å‰ç¼€ï¼ˆåŒ…æ‹¬ç©ºæ ¼ï¼‰
            author = re.sub(r'^(è½¦å‹å·|ä½œè€…|æ¥æº)\s*[ï¼š:\s]*', '', author).strip()
            # å†æ¬¡æ¸…ç†å¤šä½™ç©ºæ ¼
            author = ' '.join(author.split())
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial'
        }
        
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æœªæ‰¾åˆ°ä½œè€…',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_toutiao_playwright(url):
    """ä»Šæ—¥å¤´æ¡ä¸“ç”¨ - Playwrightæå–ï¼ˆ80%æˆåŠŸç‡ï¼‰"""
    if not PLAYWRIGHT_AVAILABLE:
        # é™çº§åˆ°requests
        return extract_general_info(url)
    
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled']
            )
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
            )
            page = context.new_page()
            
            # å»é™¤webdriveræ ‡è®°
            page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)
            
            # è®¿é—®é¡µé¢
            try:
                page.goto(url, wait_until='load', timeout=30000)
                time.sleep(3)  # ç­‰å¾…JavaScriptæ¸²æŸ“
            except:
                pass  # å³ä½¿è¶…æ—¶ä¹Ÿå°è¯•ç»§ç»­
            
            # è·å–é¡µé¢å†…å®¹
            html_content = page.content()
            browser.close()
            
            # è§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            title = None
            author = None
            
            # æå–æ ‡é¢˜ - å¤šç§æ–¹æ³•
            # æ–¹æ³•1: h1æ ‡ç­¾
            h1_elem = soup.find('h1')
            if h1_elem:
                title = h1_elem.get_text().strip()
            
            # æ–¹æ³•2: titleæ ‡ç­¾
            if not title or len(title) < 5:
                title_elem = soup.find('title')
                if title_elem:
                    title = title_elem.get_text().strip()
                    # æ¸…ç†æ ‡é¢˜ï¼ˆå»æ‰ç½‘ç«™åï¼‰
                    if '_' in title:
                        title = title.split('_')[0].strip()
                    elif '-' in title:
                        title = title.split('-')[0].strip()
            
            # æ–¹æ³•3: metaæ ‡ç­¾
            if not title or len(title) < 5:
                meta_title = soup.find('meta', {'property': 'og:title'})
                if meta_title:
                    title = meta_title.get('content', '').strip()
            
            # æå–ä½œè€… - å¤šç§æ–¹æ³•
            # æ–¹æ³•1: ä»JSONæ•°æ®ä¸­æå–
            json_patterns = [
                r'"name"\s*:\s*"([^"]{2,30})"',  # ä½œè€…å
                r'"source"\s*:\s*"([^"]{2,30})"',  # æ¥æº
                r'"author_name"\s*:\s*"([^"]{2,30})"',
                r'\"ä½œè€…\"\s*:\s*\"([^\"]{2,30})\"',
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, html_content)
                if matches:
                    # è¿‡æ»¤æ‰å¯èƒ½çš„è¯¯åŒ¹é…
                    for match in matches:
                        if match and len(match) >= 2 and len(match) <= 30:
                            # æ’é™¤å¸¸è§è¯¯åŒ¹é…
                            if match not in ['ä»Šæ—¥å¤´æ¡', 'toutiao', 'article', 'content', 'title']:
                                author = match
                                break
                    if author:
                        break
            
            # æ–¹æ³•2: ä»HTMLå…ƒç´ æå–
            if not author:
                author_selectors = [
                    {'class': re.compile('author|writer', re.I)},
                    {'class': re.compile('source', re.I)},
                    {'data-id': True},
                ]
                for selector in author_selectors:
                    elem = soup.find('span', selector) or soup.find('a', selector) or soup.find('div', selector)
                    if elem:
                        text = elem.get_text().strip()
                        if text and 2 <= len(text) <= 30:
                            author = text
                            break
            
            return {
                'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
                'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
                'status': 'success' if (title and author) else 'partial'
            }
            
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def is_baidu_or_douyin(url):
    """æ£€æŸ¥æ˜¯å¦æ˜¯ç™¾åº¦ç³»æˆ–æŠ–éŸ³é“¾æ¥"""
    url_lower = url.lower()
    return ('baijiahao.baidu.com' in url_lower or 
            'mbd.baidu.com' in url_lower or 
            'douyin.com' in url_lower or 
            'iesdouyin.com' in url_lower)

def extract_platform_info(url):
    """è¯†åˆ«å¹³å°å¹¶ä½¿ç”¨ç‰¹å®šæ–¹æ³•æå–ä¿¡æ¯"""
    url_lower = url.lower()
    
    if 'weibo.com' in url_lower:
        return extract_weibo_breakthrough(url)
    elif 'toutiao.com' in url_lower:
        # ä»Šæ—¥å¤´æ¡ä½¿ç”¨Playwright
        return extract_toutiao_playwright(url)
    elif 'douyin.com' in url_lower or 'iesdouyin.com' in url_lower:
        return extract_douyin_enhanced(url)
    elif 'xiaohongshu.com' in url_lower or 'xhslink.com' in url_lower:
        return extract_xiaohongshu_info(url)
    elif 'bilibili.com' in url_lower:
        return extract_bilibili_info(url)
    elif 'autohome.com' in url_lower:
        return extract_autohome_info(url)
    elif 'baijiahao.baidu.com' in url_lower or 'mbd.baidu.com' in url_lower:
        return extract_baidu_info(url)
    elif 'dripcar.com' in url_lower:
        return extract_dripcar_info(url)
    elif 'maiche.com' in url_lower:
        return extract_maiche_info(url)
    else:
        return extract_general_info(url)

def extract_with_playwright_browser(url):
    """ä½¿ç”¨ Playwright æµè§ˆå™¨æ–¹æ¡ˆå¤„ç†éš¾å¤„ç†çš„å¹³å°ï¼ˆç™¾åº¦ç³»ã€æŠ–éŸ³ï¼‰"""
    if not PLAYWRIGHT_AVAILABLE:
        return {
            'title': 'Playwrightæœªå®‰è£…',
            'author': 'æœªæ‰¾åˆ°',
            'status': 'failed: Playwrightæœªå®‰è£…'
        }
    
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled']
            )
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
            )
            page = context.new_page()
            
            # å»é™¤ webdriver æ ‡è®°
            page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)
            
            # è®¿é—®é¡µé¢
            try:
                page.goto(url, wait_until='load', timeout=30000)
                time.sleep(3)  # ç­‰å¾… JavaScript æ¸²æŸ“
            except:
                pass  # å³ä½¿è¶…æ—¶ä¹Ÿå°è¯•ç»§ç»­
            
            # è·å–é¡µé¢å†…å®¹
            html_content = page.content()
            browser.close()
            
            # è§£æ HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            url_lower = url.lower()
            
            title = None
            author = None
            
            # æå–æ ‡é¢˜
            # æ–¹æ³•1: h1 æ ‡ç­¾
            h1_elem = soup.find('h1')
            if h1_elem:
                title = h1_elem.get_text().strip()
            
            # æ–¹æ³•2: title æ ‡ç­¾
            if not title or len(title) < 5:
                title_elem = soup.find('title')
                if title_elem:
                    title = title_elem.get_text().strip()
                    # æ¸…ç†æ ‡é¢˜
                    for sep in ['_', ' - ', '-']:
                        if sep in title:
                            parts = title.split(sep)
                            if len(parts[0].strip()) > 5:
                                title = parts[0].strip()
                                break
            
            # æ–¹æ³•3: meta æ ‡ç­¾
            if not title or len(title) < 5:
                meta_title = soup.find('meta', {'property': 'og:title'})
                if meta_title:
                    title = meta_title.get('content', '').strip()
            
            # æå–ä½œè€…
            # é’ˆå¯¹ç™¾åº¦ç³»
            if 'baidu' in url_lower:
                # æ–¹æ³•1: HTML æ ‡ç­¾
                author_elem = soup.find('span', {'data-testid': 'author-name'})
                if author_elem:
                    author = author_elem.get_text().strip()
                
                if not author:
                    author_elem = soup.find('span', class_='_2gGWi')
                    if author_elem:
                        author = author_elem.get_text().strip()
                
                # æ–¹æ³•2: JSON æ•°æ®
                if not author:
                    author_patterns = [
                        r'"author"\s*:\s*{\s*[^}]*"name"\s*:\s*"([^"]+)"',
                        r'"authorName"\s*:\s*"([^"]{2,30})"',
                        r'"author_name"\s*:\s*"([^"]{2,30})"',
                    ]
                    for pattern in author_patterns:
                        match = re.search(pattern, html_content)
                        if match:
                            author = match.group(1)
                            break
            
            # é’ˆå¯¹æŠ–éŸ³
            elif 'douyin' in url_lower:
                # æ–¹æ³•1: JSON æ•°æ®æ·±åº¦æœç´¢
                render_match = re.search(r'<script id="RENDER_DATA" type="application/json">([^<]+)</script>', html_content)
                if render_match:
                    try:
                        json_str = html_module.unescape(render_match.group(1))
                        data = json.loads(json_str)
                        
                        def deep_find_author(obj, depth=0, max_depth=10):
                            if depth > max_depth:
                                return None
                            
                            if isinstance(obj, dict):
                                author_keys = ['nickname', 'authorName', 'unique_id', 'userName']
                                for key in author_keys:
                                    if key in obj and isinstance(obj[key], str) and 2 < len(obj[key]) < 50:
                                        return obj[key]
                                
                                for key in ['author', 'user']:
                                    if key in obj and isinstance(obj[key], dict):
                                        result = deep_find_author(obj[key], depth+1, max_depth)
                                        if result:
                                            return result
                                
                                for value in obj.values():
                                    result = deep_find_author(value, depth+1, max_depth)
                                    if result:
                                        return result
                            
                            elif isinstance(obj, list):
                                for item in obj:
                                    result = deep_find_author(item, depth+1, max_depth)
                                    if result:
                                        return result
                            
                            return None
                        
                        author = deep_find_author(data)
                    except:
                        pass
                
                # æ–¹æ³•2: æ­£åˆ™è¡¨è¾¾å¼
                if not author:
                    patterns = [
                        r'"nickname":\s*"([^"]{2,30})"',
                        r'"authorName":\s*"([^"]{2,30})"',
                    ]
                    for pattern in patterns:
                        matches = re.findall(pattern, html_content)
                        if matches:
                            author = matches[0]
                            break
            
            # é€šç”¨ä½œè€…æå–ï¼ˆfallbackï¼‰
            if not author:
                meta_author = soup.find('meta', {'name': 'author'})
                if meta_author:
                    author = meta_author.get('content', '').strip()
            
            return {
                'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
                'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
                'status': 'success (Playwright)' if (title and author) else 'partial (Playwright)'
            }
            
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_title_and_author(url):
    """ä»URLæå–æ ‡é¢˜å’Œä½œè€…ä¿¡æ¯"""
    return extract_platform_info(url)

def main():
    print("=" * 60)
    print("é“¾æ¥æ ‡é¢˜å’Œä½œè€…æå–å·¥å…· v4.6 - ä¸¤é˜¶æ®µå¤„ç†ç‰ˆ")
    print("âœ… é˜¶æ®µ1: å…ˆå¤„ç†æ™®é€šé“¾æ¥ï¼ˆè·³è¿‡ç™¾åº¦/æŠ–éŸ³ï¼‰")
    print("âœ… é˜¶æ®µ2: ç”¨Playwrightæ‰¹é‡å¤„ç†ç™¾åº¦/æŠ–éŸ³")
    print("ğŸ”´ 404é”™è¯¯æ•´è¡Œæ ‡çº¢ | ğŸŸ¡ å¤±è´¥å•å…ƒæ ¼æ ‡é»„")
    print("=" * 60)
    
    excel_file = 'æµ‹è¯•æ•°æ®é›†_éšæœºæŠ½æ ·.xlsx'
    max_links = None  # å¤„ç†å…¨éƒ¨æ•°æ®
    
    print(f"\næ­£åœ¨è¯»å–æ–‡ä»¶: {excel_file}")
    links = read_excel_with_links(excel_file)
    
    print(f"æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
    
    # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆå¯é€‰ï¼‰
    if max_links and len(links) > max_links:
        links = links[:max_links]
        print(f"âš ï¸  é™åˆ¶å¤„ç†å‰ {max_links} æ¡é“¾æ¥\n")
    else:
        print(f"å¤„ç†å…¨éƒ¨ {len(links)} æ¡é“¾æ¥\n")
    
    results = []
    delayed_links = []  # å­˜å‚¨ç™¾åº¦ç³»å’ŒæŠ–éŸ³é“¾æ¥ï¼Œå»¶è¿Ÿå¤„ç†
    start_time = time.time()
    
    # ========== é˜¶æ®µ1: å¤„ç†æ™®é€šé“¾æ¥ï¼Œè·³è¿‡ç™¾åº¦/æŠ–éŸ³ ==========
    print("\n" + "=" * 60)
    print("ã€é˜¶æ®µ1ã€‘å¤„ç†æ™®é€šé“¾æ¥ï¼ˆç™¾åº¦/æŠ–éŸ³å°†åœ¨é˜¶æ®µ2å¤„ç†ï¼‰")
    print("=" * 60 + "\n")
    
    for idx, link_info in enumerate(links, 1):
        website_name = get_website_name(link_info['url'])
        url_short = link_info['url'][:60]
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç™¾åº¦æˆ–æŠ–éŸ³
        if is_baidu_or_douyin(link_info['url']):
            print(f"[{idx}/{len(links)}] {website_name} | {url_short}... â¸ï¸  å»¶è¿Ÿå¤„ç†", flush=True)
            delayed_links.append({
                'idx': idx,
                'link_info': link_info,
                'website_name': website_name
            })
            # æ·»åŠ å ä½ç»“æœ
            results.append({
                'åŸé“¾æ¥': link_info['url'],
                'ç½‘ç«™å': website_name,
                'ä½œè€…': 'å¾…å¤„ç†',
                'æ ‡é¢˜': 'å¾…å¤„ç†',
                'çŠ¶æ€': 'pending'
            })
            continue
        
        # å¤„ç†æ™®é€šé“¾æ¥
        iter_start = time.time()
        print(f"[{idx}/{len(links)}] {website_name} | {url_short}...", end=' ', flush=True)
        
        info = extract_title_and_author(link_info['url'])
        
        iter_time = time.time() - iter_start
        status_icon = 'âœ…' if info['status'] == 'success' else ('âš ï¸' if 'partial' in info['status'] else 'âŒ')
        print(f"{status_icon} ({iter_time:.1f}s)", flush=True)
        
        results.append({
            'åŸé“¾æ¥': link_info['url'],
            'ç½‘ç«™å': website_name,
            'ä½œè€…': info['author'],
            'æ ‡é¢˜': info['title'],
            'çŠ¶æ€': info['status']
        })
        
        # æ¯10æ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ç»Ÿè®¡
        if idx % 10 == 0:
            elapsed = time.time() - start_time
            processed = idx - len(delayed_links)
            if processed > 0:
                avg_time = elapsed / processed
                remaining_normal = (len(links) - idx) * avg_time
                success = sum(1 for r in results if 'success' in r['çŠ¶æ€'])
                print(f"  â±ï¸  å·²ç”¨{elapsed/60:.1f}åˆ† | é¢„è®¡å‰©ä½™{remaining_normal/60:.1f}åˆ† | æˆåŠŸç‡{success/processed*100:.1f}% | å»¶è¿Ÿ{len(delayed_links)}ä¸ª\n")
        
        if idx < len(links):
            time.sleep(0.5)
    
    # ========== é˜¶æ®µ2: ç”¨ Playwright å¤„ç†å»¶è¿Ÿçš„é“¾æ¥ ==========
    if delayed_links:
        print("\n" + "=" * 60)
        print(f"ã€é˜¶æ®µ2ã€‘ä½¿ç”¨Playwrightå¤„ç†å»¶è¿Ÿçš„{len(delayed_links)}ä¸ªé“¾æ¥")
        print("=" * 60 + "\n")
        
        stage2_start = time.time()
        
        for delayed_idx, delayed_item in enumerate(delayed_links, 1):
            idx = delayed_item['idx']
            link_info = delayed_item['link_info']
            website_name = delayed_item['website_name']
            url_short = link_info['url'][:60]
            
            iter_start = time.time()
            print(f"[{delayed_idx}/{len(delayed_links)}] {website_name} | {url_short}...", end=' ', flush=True)
            
            # ä½¿ç”¨ Playwright å¤„ç†
            info = extract_with_playwright_browser(link_info['url'])
            
            iter_time = time.time() - iter_start
            status_icon = 'âœ…' if 'success' in info['status'] else ('âš ï¸' if 'partial' in info['status'] else 'âŒ')
            print(f"{status_icon} ({iter_time:.1f}s)", flush=True)
            
            # æ›´æ–°resultsä¸­å¯¹åº”ä½ç½®çš„ç»“æœ
            results[idx - 1] = {
                'åŸé“¾æ¥': link_info['url'],
                'ç½‘ç«™å': website_name,
                'ä½œè€…': info['author'],
                'æ ‡é¢˜': info['title'],
                'çŠ¶æ€': info['status']
            }
            
            # æ¯5æ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if delayed_idx % 5 == 0 or delayed_idx == len(delayed_links):
                elapsed = time.time() - stage2_start
                avg_time = elapsed / delayed_idx
                remaining = (len(delayed_links) - delayed_idx) * avg_time
                success = sum(1 for i in range(delayed_idx) if 'success' in results[delayed_links[i]['idx'] - 1]['çŠ¶æ€'])
                print(f"  â±ï¸  é˜¶æ®µ2å·²ç”¨{elapsed/60:.1f}åˆ† | é¢„è®¡å‰©ä½™{remaining/60:.1f}åˆ† | æˆåŠŸç‡{success/delayed_idx*100:.1f}%\n")
            
            # Playwright å¤„ç†éœ€è¦æ›´é•¿å»¶è¿Ÿ
            if delayed_idx < len(delayed_links):
                time.sleep(2)
    
    df = pd.DataFrame(results)
    output_file = 'é“¾æ¥åˆ†æç»“æœ_v4_æœ€ç»ˆç‰ˆ.xlsx'
    df.to_excel(output_file, index=False, engine='openpyxl')
    
    # æ·»åŠ é¢œè‰²æ ‡è®°
    print("\næ·»åŠ é¢œè‰²æ ‡è®°...")
    wb = openpyxl.load_workbook(output_file)
    ws = wb.active
    
    # å®šä¹‰é¢œè‰²
    red_fill = PatternFill(start_color='FFCCCC', end_color='FFCCCC', fill_type='solid')  # æµ…çº¢è‰²
    yellow_fill = PatternFill(start_color='FFFF99', end_color='FFFF99', fill_type='solid')  # æµ…é»„è‰²
    
    # éå†æ•°æ®è¡Œï¼ˆä»ç¬¬2è¡Œå¼€å§‹ï¼Œç¬¬1è¡Œæ˜¯è¡¨å¤´ï¼‰
    for idx, row_data in enumerate(results, start=2):
        status = row_data['çŠ¶æ€'].lower()
        title = row_data['æ ‡é¢˜']
        author = row_data['ä½œè€…']
        
        # 404é”™è¯¯ â†’ æ•´è¡Œæ ‡çº¢
        if '404' in status or '404' in title:
            for col in range(1, 6):  # A-Eåˆ—ï¼ˆåŸé“¾æ¥ã€ç½‘ç«™åã€ä½œè€…ã€æ ‡é¢˜ã€çŠ¶æ€ï¼‰
                ws.cell(row=idx, column=col).fill = red_fill
        else:
            # éƒ¨åˆ†å¤±è´¥ â†’ åªæ ‡è®°å¤±è´¥çš„å•å…ƒæ ¼ä¸ºé»„è‰²
            # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦å¤±è´¥
            if 'æœªæ‰¾åˆ°' in title or 'æå–å¤±è´¥' in title:
                ws.cell(row=idx, column=4).fill = yellow_fill  # Dåˆ—ï¼šæ ‡é¢˜
            
            # æ£€æŸ¥ä½œè€…æ˜¯å¦å¤±è´¥
            if 'æœªæ‰¾åˆ°' in author or 'æå–å¤±è´¥' in author:
                ws.cell(row=idx, column=3).fill = yellow_fill  # Cåˆ—ï¼šä½œè€…
    
    wb.save(output_file)
    
    print("\n" + "=" * 60)
    print(f"åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("ğŸ”´ 404é”™è¯¯ â†’ æ•´è¡Œæ ‡çº¢")
    print("ğŸŸ¡ æ ‡é¢˜/ä½œè€…æå–å¤±è´¥ â†’ å¯¹åº”å•å…ƒæ ¼æ ‡é»„")
    print("=" * 60)
    
    success_count = sum(1 for r in results if 'success' in r['çŠ¶æ€'].lower())
    partial_count = sum(1 for r in results if 'partial' in r['çŠ¶æ€'].lower())
    failed_count = len(results) - success_count - partial_count
    
    print(f"\nã€ç»Ÿè®¡ã€‘")
    print(f"æ€»é“¾æ¥æ•°: {len(results)}")
    print(f"å®Œå…¨æˆåŠŸ: {success_count} ({success_count/len(results)*100:.1f}%)")
    print(f"éƒ¨åˆ†æˆåŠŸ: {partial_count} ({partial_count/len(results)*100:.1f}%)")
    print(f"å¤±è´¥: {failed_count} ({failed_count/len(results)*100:.1f}%)")
    
    print(f"\nã€å¹³å°åˆ†å¸ƒã€‘")
    platform_count = {}
    for r in results:
        platform = r['ç½‘ç«™å']
        platform_count[platform] = platform_count.get(platform, 0) + 1
    
    for platform, count in sorted(platform_count.items(), key=lambda x: x[1], reverse=True):
        print(f"{platform}: {count}ä¸ª")
    
    print("\nå¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()

