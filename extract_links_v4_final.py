"""
é“¾æ¥æ ‡é¢˜å’Œä½œè€…æå–å·¥å…· v4.5 - ç²¾å‡†é¢œè‰²æ ‡è®°ç‰ˆ
âœ… æˆåŠŸçªç ´å¾®åšï¼(98.2%)
âœ… æŠ–éŸ³80%æˆåŠŸç‡
âœ… å°çº¢ä¹¦90%æˆåŠŸç‡
âœ… æ±½è½¦ä¹‹å®¶90%æˆåŠŸç‡
âœ… ç™¾å®¶å·66%+æˆåŠŸç‡ï¼ˆPlaywrightç»•è¿‡å®‰å…¨éªŒè¯ï¼‰
âœ… ä»Šæ—¥å¤´æ¡80%æˆåŠŸç‡ï¼ˆè‡ªåŠ¨Playwrightï¼‰
âœ… Excelç²¾å‡†é¢œè‰²æ ‡è®°ï¼š
   ğŸ”´ 404é”™è¯¯ â†’ æ•´è¡Œæ ‡çº¢
   ğŸŸ¡ æ ‡é¢˜/ä½œè€…å¤±è´¥ â†’ å¯¹åº”å•å…ƒæ ¼æ ‡é»„
"""
import pandas as pd
import requests
from bs4 import BeautifulSoup
import openpyxl
from openpyxl.styles import PatternFill
import time
import re
import json
import html as html_module
import os
import sys

# è®¾ç½®Playwrightæµè§ˆå™¨è·¯å¾„ï¼ˆç”¨äºæ‰“åŒ…åçš„exeï¼‰
if getattr(sys, 'frozen', False):
    # å¦‚æœæ˜¯æ‰“åŒ…åçš„exe
    base_path = sys._MEIPASS
    playwright_browser_path = os.path.join(base_path, 'playwright_browsers')
    if os.path.exists(playwright_browser_path):
        os.environ['PLAYWRIGHT_BROWSERS_PATH'] = playwright_browser_path
        print(f"âœ… ä½¿ç”¨æ‰“åŒ…çš„Playwrightæµè§ˆå™¨: {playwright_browser_path}")

# å°è¯•å¯¼å…¥Playwrightï¼ˆä»Šæ—¥å¤´æ¡ä¸“ç”¨ï¼‰
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸  Playwrightæœªå®‰è£…ï¼Œä»Šæ—¥å¤´æ¡å°†ä½¿ç”¨requestsï¼ˆæ•ˆæœè¾ƒå·®ï¼‰")

# åˆ›å»ºå…¨å±€Session
session = requests.Session()

# æ‡‚è½¦å¸è°ƒè¯•æ—¥å¿—æ–‡ä»¶è·¯å¾„
DCD_DEBUG_LOG_FILE = 'dongchedi_debug.log'
_dcd_log_initialized = False

def dcd_debug_log(message):
    """æ‡‚è½¦å¸è°ƒè¯•æ—¥å¿—ï¼šåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    global _dcd_log_initialized
    
    # é¦–æ¬¡è°ƒç”¨æ—¶æ¸…ç©ºæ—§æ—¥å¿—
    if not _dcd_log_initialized:
        try:
            with open(DCD_DEBUG_LOG_FILE, 'w', encoding='utf-8') as f:
                f.write(f"=== æ‡‚è½¦å¸è°ƒè¯•æ—¥å¿— - {time.strftime('%Y-%m-%d %H:%M:%S')} ===\n\n")
            print(f"\nğŸ’¾ æ‡‚è½¦å¸è°ƒè¯•æ—¥å¿—å°†ä¿å­˜åˆ°: {DCD_DEBUG_LOG_FILE}\n", flush=True)
        except:
            pass
        _dcd_log_initialized = True
    
    timestamp = time.strftime("%H:%M:%S")
    log_message = f"[{timestamp}] {message}"
    print(log_message, flush=True)
    
    try:
        with open(DCD_DEBUG_LOG_FILE, 'a', encoding='utf-8') as f:
            f.write(log_message + '\n')
    except:
        pass

def read_excel_with_links(file_path):
    """è¯»å–Excelæ–‡ä»¶å¹¶æå–æ‰€æœ‰é“¾æ¥"""
    wb = openpyxl.load_workbook(file_path)
    ws = wb.active
    
    links = []
    for row in ws.iter_rows():
        for cell in row:
            if cell.hyperlink:
                links.append({
                    'cell': cell.coordinate,
                    'text': cell.value,
                    'url': cell.hyperlink.target
                })
            elif isinstance(cell.value, str) and ('http://' in cell.value or 'https://' in cell.value):
                urls = re.findall(r'https?://[^\s]+', cell.value)
                for url in urls:
                    links.append({
                        'cell': cell.coordinate,
                        'text': cell.value,
                        'url': url
                    })
    
    return links

def get_website_name(url):
    """ä»URLæå–ç½‘ç«™åç§°"""
    url_lower = url.lower()
    
    if 'bilibili.com' in url_lower:
        return 'å“”å“©å“”å“©'
    elif 'douyin.com' in url_lower or 'iesdouyin.com' in url_lower:
        return 'æŠ–éŸ³'
    elif 'xiaohongshu.com' in url_lower or 'xhslink.com' in url_lower:
        return 'å°çº¢ä¹¦'
    elif 'mp.weixin.qq' in url_lower:
        return 'å¾®ä¿¡å…¬ä¼—å·'
    elif 'm.weibo' in url_lower or 'weibo.com' in url_lower:
        return 'æ–°æµªå¾®åš'
    elif 'toutiao.com' in url_lower:
        return 'ä»Šæ—¥å¤´æ¡'
    elif 'autohome.com' in url_lower:
        return 'æ±½è½¦ä¹‹å®¶'
    elif 'vc.yiche' in url_lower or 'sv.m.yiche' in url_lower or 'vc.m.yiche' in url_lower or 'yiche.com' in url_lower:
        return 'æ˜“è½¦'
    elif '163.com' in url_lower or '.163.' in url_lower:
        return 'ç½‘æ˜“'
    elif 'mbd.baidu' in url_lower or 'baijiahao.baidu' in url_lower:
        return 'ç™¾åº¦'
    elif 'yidianzixun' in url_lower:
        return 'ä¸€ç‚¹èµ„è®¯'
    elif 'zjbyte.cn' in url_lower or 'dongchedi' in url_lower or 'dcd' in url_lower:
        return 'æ‡‚è½¦å¸'
    elif 'zhihu.com' in url_lower:
        return 'çŸ¥ä¹'
    else:
        try:
            from urllib.parse import urlparse
            domain = urlparse(url).netloc
            domain = domain.replace('www.', '').replace('.com', '').replace('.cn', '')
            return domain if domain else 'æœªçŸ¥ç½‘ç«™'
        except:
            return 'æœªçŸ¥ç½‘ç«™'

def extract_weibo_breakthrough(url):
    """å¾®åšçªç ´ç‰ˆ - ä½¿ç”¨ç§»åŠ¨ç«¯API"""
    try:
        # ä»URLæå–mid
        id_match = re.search(r'/(\d+)/([A-Za-z0-9]+)', url)
        if not id_match:
            return {
                'title': 'URLæ ¼å¼é”™è¯¯',
                'author': 'URLæ ¼å¼é”™è¯¯',
                'status': 'failed: URLæ ¼å¼ä¸æ­£ç¡®'
            }
        
        uid, mid = id_match.groups()
        
        # ä½¿ç”¨ç§»åŠ¨ç«¯API
        api_url = f"https://m.weibo.cn/statuses/show?id={mid}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15',
            'Accept': 'application/json',
            'Referer': f'https://m.weibo.cn/status/{mid}',
            'X-Requested-With': 'XMLHttpRequest',
        }
        
        response = session.get(api_url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            try:
                data = response.json()
                
                if 'data' in data:
                    status = data['data']
                    
                    # æå–æ ‡é¢˜ - ä¼˜å…ˆä½¿ç”¨text_rawï¼Œå¦åˆ™ä½¿ç”¨text
                    title = status.get('text_raw', status.get('text', ''))
                    
                    # æ¸…ç†HTMLæ ‡ç­¾
                    if title:
                        title = re.sub(r'<[^>]+>', '', title)
                        title = title.strip()
                        # é™åˆ¶é•¿åº¦
                        if len(title) > 200:
                            title = title[:200] + '...'
                    
                    # æå–ä½œè€…
                    author = ''
                    if 'user' in status:
                        author = status['user'].get('screen_name', '')
                    
                    return {
                        'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
                        'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
                        'status': 'success' if (title and author) else 'partial (å¾®åšAPIæå–)'
                    }
                else:
                    return {
                        'title': 'APIå“åº”æ— æ•°æ®',
                        'author': 'æœªæ‰¾åˆ°',
                        'status': 'failed: APIå“åº”æ ¼å¼é”™è¯¯'
                    }
            
            except json.JSONDecodeError:
                return {
                    'title': 'APIå“åº”è§£æå¤±è´¥',
                    'author': 'æœªæ‰¾åˆ°',
                    'status': 'failed: JSONè§£æé”™è¯¯'
                }
        else:
            return {
                'title': f'APIè¯·æ±‚å¤±è´¥({response.status_code})',
                'author': 'æœªæ‰¾åˆ°',
                'status': f'failed: HTTP {response.status_code}'
            }
    
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_douyin_enhanced(url):
    """æŠ–éŸ³å¢å¼ºæå– - æ·±åº¦JSONæœç´¢"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15',
            'Accept': 'text/html,application/xhtml+xml',
            'Referer': 'https://www.douyin.com/',
        }
        
        response = session.get(url, headers=headers, timeout=15, allow_redirects=True)
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title = None
        author = None
        
        # æ–¹æ³•1ï¼šä»titleæ ‡ç­¾æå–æ ‡é¢˜å’Œä½œè€…
        if soup.find('title'):
            title_text = soup.find('title').text.strip()
            # æŠ–éŸ³titleæ ¼å¼é€šå¸¸æ˜¯ï¼š"æ ‡é¢˜ - ä½œè€… - æŠ–éŸ³" æˆ– "æ ‡é¢˜ @ä½œè€… æŠ–éŸ³"
            if ' - ' in title_text:
                parts = title_text.split(' - ')
                if len(parts) >= 2:
                    title = parts[0].strip()
                    # ç¬¬äºŒéƒ¨åˆ†å¯èƒ½æ˜¯ä½œè€…
                    potential_author = parts[1].replace('æŠ–éŸ³', '').strip()
                    if potential_author and len(potential_author) > 1 and 'æŠ–éŸ³' not in potential_author:
                        author = potential_author
            elif '@' in title_text:
                # æ ¼å¼å¦‚ï¼š"æ ‡é¢˜ @ä½œè€…"
                match = re.search(r'@([^\s@]+)', title_text)
                if match:
                    author = match.group(1)
                title = title_text.split('@')[0].strip()
            else:
                title = title_text.replace('- æŠ–éŸ³', '').replace('æŠ–éŸ³', '').strip()
        
        # æ–¹æ³•2ï¼šä»RENDER_DATAæ·±åº¦æå–
        render_match = re.search(r'<script id="RENDER_DATA" type="application/json">([^<]+)</script>', response.text)
        if render_match:
            try:
                json_str = html_module.unescape(render_match.group(1))
                data = json.loads(json_str)
                
                # è¶…æ·±åº¦é€’å½’æœç´¢ï¼ˆå¢å¼ºç‰ˆï¼‰
                def deep_find_author(obj, depth=0, max_depth=12):
                    if depth > max_depth:
                        return None
                    
                    if isinstance(obj, dict):
                        # æ‰©å±•ä½œè€…å­—æ®µ
                        author_keys = ['nickname', 'authorName', 'unique_id', 'short_id', 
                                     'userName', 'user_name', 'name', 'author_name',
                                     'secUid', 'uniqueId', 'screen_name']
                        for key in author_keys:
                            if key in obj and isinstance(obj[key], str):
                                value = obj[key].strip()
                                # æ’é™¤æ˜æ˜¾ä¸æ˜¯ä½œè€…çš„å€¼
                                if (2 < len(value) < 50 and 
                                    value not in ['æŠ–éŸ³', 'douyin', 'video', 'image'] and
                                    not value.startswith('http')):
                                    return value
                        
                        # ä¼˜å…ˆæœç´¢authorå’Œuserå­—æ®µ
                        for key in ['author', 'user', 'authorInfo', 'userInfo']:
                            if key in obj and isinstance(obj[key], dict):
                                result = deep_find_author(obj[key], depth+1, max_depth)
                                if result:
                                    return result
                        
                        # éå†æ‰€æœ‰å€¼
                        for value in obj.values():
                            result = deep_find_author(value, depth+1, max_depth)
                            if result:
                                return result
                    
                    elif isinstance(obj, list):
                        for item in obj:
                            result = deep_find_author(item, depth+1, max_depth)
                            if result:
                                return result
                    
                    return None
                
                if not author:
                    author = deep_find_author(data)
                
                # åŒæ—¶å°è¯•æå–æ ‡é¢˜
                if not title:
                    def deep_find_title(obj, depth=0, max_depth=10):
                        if depth > max_depth:
                            return None
                        if isinstance(obj, dict):
                            for key in ['title', 'desc', 'description', 'content']:
                                if key in obj and isinstance(obj[key], str):
                                    value = obj[key].strip()
                                    if 10 < len(value) < 500:
                                        return value
                            for value in obj.values():
                                result = deep_find_title(value, depth+1, max_depth)
                                if result:
                                    return result
                        elif isinstance(obj, list):
                            for item in obj:
                                result = deep_find_title(item, depth+1, max_depth)
                                if result:
                                    return result
                        return None
                    
                    title = deep_find_title(data)
                
            except Exception as e:
                print(f"  [æŠ–éŸ³JSONè§£æé”™è¯¯: {str(e)[:30]}]", flush=True)
        
        # æ–¹æ³•3ï¼šæ‰©å±•æ­£åˆ™è¡¨è¾¾å¼æœç´¢
        if not author:
            patterns = [
                r'"nickname"\s*:\s*"([^"]{2,30})"',
                r'"authorName"\s*:\s*"([^"]{2,30})"',
                r'"uniqueId"\s*:\s*"([^"]{2,30})"',
                r'"user_name"\s*:\s*"([^"]{2,30})"',
                r'"name"\s*:\s*"([^"]{2,30})"',
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, response.text)
                if matches:
                    for match in matches:
                        # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ä½œè€…å
                        if (re.match(r'^[\u4e00-\u9fa5a-zA-Z0-9_\-]+$', match) and
                            match not in ['æŠ–éŸ³', 'douyin', 'video'] and
                            len(match) > 1):
                            author = match
                            break
                if author:
                    break
        
        # æ–¹æ³•4ï¼šä»metaæ ‡ç­¾æå–
        if not author:
            meta_author = soup.find('meta', {'name': 'author'})
            if meta_author:
                author = meta_author.get('content', '').strip()
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial (æŠ–éŸ³ä½œè€…éš¾æå–)'
        }
        
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_weixin_info(url):
    """å¾®ä¿¡å…¬ä¼—å·æå–"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15',
        }
        
        response = session.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title = None
        author = None  # è¿™é‡Œçš„authoræŒ‡å…¬ä¼—å·åç§°
        
        # æå–æ ‡é¢˜
        # æ–¹æ³•1: metaæ ‡ç­¾
        meta_title = soup.find('meta', {'property': 'og:title'})
        if meta_title:
            title = meta_title.get('content', '').strip()
        
        # æ–¹æ³•2: titleæ ‡ç­¾
        if not title:
            title_elem = soup.find('title')
            if title_elem:
                title = title_elem.get_text().strip()
        
        # æå–å…¬ä¼—å·åç§°ï¼ˆä½œè€…å­—æ®µï¼‰
        # æ–¹æ³•1: id="js_name"ï¼ˆæœ€å‡†ç¡®ï¼‰
        js_name_elem = soup.find(id='js_name')
        if js_name_elem:
            author = js_name_elem.get_text().strip()
        
        # æ–¹æ³•2: class="rich_media_meta_nickname"
        if not author:
            nickname_elem = soup.find(class_='rich_media_meta_nickname')
            if nickname_elem:
                author = nickname_elem.get_text().strip()
        
        # æ–¹æ³•3: classåŒ…å«profile_nickname
        if not author:
            profile_elem = soup.find(class_=re.compile('profile_nickname', re.I))
            if profile_elem:
                author = profile_elem.get_text().strip()
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°å…¬ä¼—å·',
            'status': 'success' if (title and author) else 'partial'
        }
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_xiaohongshu_info(url):
    """å°çº¢ä¹¦æå– - å¢å¼ºç‰ˆï¼ˆæ”¯æŒexploreé“¾æ¥ï¼‰"""
    try:
        # æ£€æµ‹æ˜¯å¦æ˜¯exploreé“¾æ¥ï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
        is_explore = '/explore/' in url.lower()
        
        # å¯¹äºexploreé“¾æ¥ï¼Œä½¿ç”¨æ›´å®Œæ•´çš„ç§»åŠ¨ç«¯headers
        if is_explore:
            headers = {
                'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/8.0.38(0x1800262c) NetType/WIFI Language/zh_CN',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9',
                'Referer': 'https://www.xiaohongshu.com/',
            }
        else:
            headers = {
                'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15',
            }
        
        response = session.get(url, headers=headers, timeout=15, allow_redirects=True)
        response.encoding = 'utf-8'
        
        # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°é”™è¯¯é¡µé¢
        if 'website-login/error' in response.url or 'error_code' in response.url:
            return {
                'title': 'è®¿é—®å—é™',
                'author': 'è®¿é—®å—é™',
                'status': 'failed: å°çº¢ä¹¦è®¿é—®å—é™ï¼Œå»ºè®®ä½¿ç”¨xhslink.comçŸ­é“¾æ¥'
            }
        
        title = None
        author = None
        
        patterns = [
            r'window\.__INITIAL_STATE__\s*=\s*({.+?})\s*<\/script>',
            r'window\.__SETUP_SERVER_STATE__\s*=\s*({.+?})\s*<\/script>',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, response.text, re.DOTALL)
            if matches:
                try:
                    data_str = matches[0].replace('\\u002F', '/')
                    
                    # æå–descå­—æ®µ
                    desc_text = None
                    desc_match = re.search(r'"desc"\s*:\s*"([^"]+)"', data_str)
                    if desc_match and len(desc_match.group(1)) > 5:
                        desc_text = desc_match.group(1)
                        # æ¸…ç†è¯é¢˜æ ‡ç­¾ï¼š#xxx[è¯é¢˜]# æˆ– #xxx#
                        desc_text = re.sub(r'#[^#\[]+\[è¯é¢˜\]#', '', desc_text)
                        desc_text = re.sub(r'#[^#]+#', '', desc_text)
                        desc_text = desc_text.strip()
                    
                    # æå–titleå­—æ®µ
                    title_text = None
                    title_match = re.search(r'"title"\s*:\s*"([^"]+)"', data_str)
                    if title_match and title_match.group(1):
                        title_text = title_match.group(1).strip()
                        if title_text == 'å°çº¢ä¹¦' or len(title_text) <= 5:
                            title_text = None
                    
                    # æ™ºèƒ½é€‰æ‹©æ ‡é¢˜ï¼š
                    # 1. å¦‚æœtitleå­˜åœ¨ä¸”é•¿åº¦åˆç†ï¼ˆ8-50å­—ï¼‰ï¼Œæ£€æŸ¥ä¸descçš„ç›¸å…³æ€§
                    # 2. å¦‚æœtitleå’Œdescä¸ç›¸å…³ï¼ˆå¯èƒ½æ˜¯æ¨èå†…å®¹çš„æ ‡é¢˜ï¼‰ï¼Œç”¨desc
                    # 3. å¦‚æœtitleå¤ªçŸ­æˆ–ä¸ºç©ºï¼Œç”¨desc
                    if title_text and desc_text:
                        title_len = len(title_text)
                        # titleé•¿åº¦åˆç†ï¼ˆ8-50å­—ï¼‰
                        if 8 <= title_len <= 50:
                            # ç®€å•æ£€æŸ¥ï¼šæå–titleå’Œdescä¸­çš„3-4å­—è¿ç»­å­—ç¬¦ä¸²ï¼Œçœ‹æ˜¯å¦æœ‰äº¤é›†
                            # å¦‚æœæœ‰å…±åŒçš„3-4å­—è¯ï¼Œè¯´æ˜ç›¸å…³
                            title_trigrams = set()
                            desc_trigrams = set()
                            
                            # æå–titleä¸­çš„3-4å­—ç‰‡æ®µï¼ˆæ’é™¤å¸¸ç”¨2å­—è¯ï¼‰
                            for i in range(len(title_text) - 2):
                                for j in range(3, 5):
                                    if i + j <= len(title_text):
                                        fragment = title_text[i:i+j]
                                        # æ’é™¤çº¯æ ‡ç‚¹æˆ–ç©ºæ ¼
                                        if any('\u4e00' <= c <= '\u9fff' for c in fragment):
                                            title_trigrams.add(fragment)
                            
                            # æå–descå‰50å­—ä¸­çš„3-4å­—ç‰‡æ®µ
                            desc_sample = desc_text[:50]
                            for i in range(len(desc_sample) - 2):
                                for j in range(3, 5):
                                    if i + j <= len(desc_sample):
                                        fragment = desc_sample[i:i+j]
                                        # æ’é™¤çº¯æ ‡ç‚¹æˆ–ç©ºæ ¼
                                        if any('\u4e00' <= c <= '\u9fff' for c in fragment):
                                            desc_trigrams.add(fragment)
                            
                            # å¦‚æœæœ‰å…±åŒçš„3-4å­—ç‰‡æ®µï¼Œè¯´æ˜ç›¸å…³ï¼Œç”¨title
                            if title_trigrams & desc_trigrams:
                                title = title_text
                            else:
                                # æ²¡æœ‰å…±åŒç‰‡æ®µï¼Œtitleå¯èƒ½æ˜¯æ¨èå†…å®¹ï¼Œç”¨desc
                                title = desc_text
                        else:
                            # titleå¤ªçŸ­ï¼Œç”¨desc
                            title = desc_text
                    elif title_text:
                        # åªæœ‰titleæ²¡æœ‰desc
                        title = title_text
                    else:
                        # æ²¡æœ‰titleï¼Œç”¨desc
                        title = desc_text
                    
                    # æå–ä½œè€…ï¼ˆåŒæ—¶æ”¯æŒ nickName å’Œ nicknameï¼‰
                    # ä¼˜å…ˆä» noteData.user.nickName æå–ï¼ˆæœ€å‡†ç¡®ï¼‰
                    user_nickname_match = re.search(r'"user"\s*:\s*{[^}]{0,500}"nickName"\s*:\s*"([^"]{2,30})"', data_str)
                    if user_nickname_match:
                        author = user_nickname_match.group(1)
                    
                    # å¤‡é€‰ï¼šé€šç”¨nicknameå­—æ®µ
                    if not author:
                        nickname_matches = re.findall(r'"nickname"\s*:\s*"([^"]+)"', data_str)
                        if nickname_matches:
                            author = nickname_matches[0]
                    
                    if title and author:
                        break
                except:
                    continue
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial'
        }
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_bilibili_info(url):
    """Bç«™æå–"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://www.bilibili.com/',
        }
        
        response = session.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title = None
        author = None
        
        meta_title = soup.find('meta', {'property': 'og:title'})
        if meta_title:
            title = meta_title.get('content', '').strip()
            for suffix in ['_å“”å“©å“”å“©_bilibili', ' - å“”å“©å“”å“©']:
                if suffix in title:
                    title = title.split(suffix)[0].strip()
        
        meta_author = soup.find('meta', {'name': 'author'})
        if meta_author:
            author = meta_author.get('content', '').strip()
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial'
        }
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_autohome_info(url):
    """æ±½è½¦ä¹‹å®¶ä¸“ç”¨æå–"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        }
        
        response = session.get(url, headers=headers, timeout=15, allow_redirects=True)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title = None
        author = None
        
        # æå–æ ‡é¢˜
        title_elem = soup.find('title')
        if title_elem:
            title = title_elem.text.strip()
        
        # è½¦å®¶å·ï¼ˆPCå’Œç§»åŠ¨ç‰ˆï¼‰ï¼šä»JSONæå–ä½œè€…ï¼ˆä¼˜å…ˆæ–¹æ¡ˆï¼‰
        if 'chejiahao' in url or ('m.autohome.com.cn' in url and 'info' in url):
            # æ–¹æ³•1: ä¼˜å…ˆä»authorNameå­—æ®µæå–ï¼ˆæœ€ç²¾å‡†ï¼Œè½¦å®¶å·ä¸“ç”¨ï¼‰
            author_match = re.search(r'"authorName"\s*:\s*"([^"]{2,30})"', response.text)
            if author_match:
                potential_author = author_match.group(1)
                if potential_author not in ['æ±½è½¦ä¹‹å®¶', 'autohome', 'è½¦å®¶å·']:
                    author = potential_author
            
            # æ–¹æ³•2: ä»åŒ…å«authoridçš„authorå¯¹è±¡ä¸­æå–
            if not author:
                author_match = re.search(r'"author"\s*:\s*{\s*[^}]*"authorid"\s*:\s*"[^"]+"\s*[^}]*"name"\s*:\s*"([^"]{2,30})"', response.text)
                if author_match:
                    author = author_match.group(1)
            
            # æ–¹æ³•3: ä»authorInfoä¸­æå–
            if not author:
                author_match = re.search(r'"authorInfo"\s*:\s*{\s*[^}]*"name"\s*:\s*"([^"]{2,30})"', response.text)
                if author_match:
                    author = author_match.group(1)
            
            # æ–¹æ³•4: ä»æ–‡ç« æ•°æ®ä¸­çš„userNameæå–
            if not author:
                author_match = re.search(r'"userName"\s*:\s*"([^"]{2,30})"', response.text)
                if author_match:
                    potential_author = author_match.group(1)
                    if potential_author not in ['æ±½è½¦ä¹‹å®¶', 'autohome', 'è½¦å®¶å·']:
                        author = potential_author
            
            # æ–¹æ³•5: ä»HTMLå…ƒç´ æå–ï¼ˆclass="authorMes"ï¼‰
            if not author:
                author_elem = soup.find('div', class_='authorMes')
                if author_elem:
                    # æŸ¥æ‰¾æ‰€æœ‰aæ ‡ç­¾ï¼Œæ‰¾åˆ°åŒ…å«ä½œè€…åçš„é‚£ä¸ªï¼ˆä¸æ˜¯ç©ºçš„ï¼Œä¸”æœ‰æ–‡æœ¬ï¼‰
                    a_elems = author_elem.find_all('a')
                    for a in a_elems:
                        text = a.get_text(strip=True)
                        if text and len(text) >= 2 and len(text) <= 30:
                            # æ’é™¤æ•°å­—ã€"å…³æ³¨"ã€"ä½œå“"ç­‰
                            if not re.match(r'^\d+', text) and text not in ['æ±½è½¦ä¹‹å®¶', 'è½¦å®¶å·', 'å…³æ³¨', 'ä½œå“']:
                                author = text
                                break
            
            # æ–¹æ³•6: æŸ¥æ‰¾classåŒ…å«authorçš„å…ƒç´ 
            if not author:
                author_elems = soup.find_all(class_=re.compile('author', re.I))
                for elem in author_elems:
                    text = elem.get_text(strip=True)
                    if text and 2 <= len(text) <= 30 and text not in ['æ±½è½¦ä¹‹å®¶', 'è½¦å®¶å·', 'ç›¸å…³æ¨è']:
                        author = text
                        break
        
        # è®ºå›å¸–å­ï¼šå°è¯•å¤šç§æ–¹æ³•
        elif 'club.autohome.com.cn' in url:
            # æ–¹æ³•1: ä»JavaScriptå˜é‡__TOPICINFO__æå–topicMemberName
            topic_match = re.search(r'topicMemberName:\s*[\'"]([^\'"]+)[\'"]', response.text)
            if topic_match:
                author = topic_match.group(1)
            
            # æ–¹æ³•2: æŸ¥æ‰¾ç”¨æˆ·é“¾æ¥
            if not author:
                user_link = soup.find('a', href=re.compile(r'/space/\d+'))
                if user_link:
                    author = user_link.get_text(strip=True)
            
            # æ–¹æ³•3: åœ¨JSONä¸­æŸ¥æ‰¾userName
            if not author:
                author_match = re.search(r'"userName":\s*"([^"]+)"', response.text)
                if author_match:
                    author = author_match.group(1)
        
        # æ–°é—»é¡µé¢ï¼šæå–ä½œè€…
        elif 'www.autohome.com.cn/news' in url:
            # æ–¹æ³•1: ä»metaæ ‡ç­¾
            meta_author = soup.find('meta', {'name': 'author'})
            if meta_author:
                author = meta_author.get('content', '').strip()
            
            # æ–¹æ³•2: æŸ¥æ‰¾classåŒ…å«authorçš„å…ƒç´ 
            if not author:
                author_elem = soup.find(class_=re.compile('author|writer', re.I))
                if author_elem:
                    author = author_elem.get_text(strip=True)
            
            # æ–¹æ³•3: åœ¨é¡µé¢æºç ä¸­æœç´¢
            if not author:
                author_patterns = [
                    r'"author"\s*:\s*"([^"]+)"',
                    r'ä½œè€…[ï¼š:]\s*([^\s<>"]+)',
                ]
                for pattern in author_patterns:
                    match = re.search(pattern, response.text)
                    if match:
                        author = match.group(1)
                        break
        
        # é€šç”¨æå–ï¼ˆfallbackï¼‰
        if not author:
            meta_author = soup.find('meta', {'name': 'author'})
            if meta_author:
                author = meta_author.get('content', '').strip()
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial'
        }
        
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_general_info(url):
    """é€šç”¨æå–"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        }
        
        response = session.get(url, headers=headers, timeout=15, allow_redirects=True)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title = None
        author = None
        
        meta_title = soup.find('meta', {'property': 'og:title'}) or soup.find('meta', {'name': 'title'})
        if meta_title:
            title = meta_title.get('content', '').strip()
        
        if not title and soup.find('title'):
            title = soup.find('title').text.strip()
        
        meta_author = soup.find('meta', {'property': 'og:author'}) or soup.find('meta', {'name': 'author'})
        if meta_author:
            author = meta_author.get('content', '').strip()
        
        if not author:
            author_elems = soup.find_all(class_=re.compile(r'author|writer', re.I))
            for elem in author_elems:
                text = elem.text.strip()
                if text and len(text) < 50:
                    author = text
                    break
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial'
        }
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_baidu_info(url):
    """ç™¾åº¦ç³»å¹³å°æå–ï¼ˆç™¾å®¶å·ã€mbd.baiduï¼‰"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml',
        }
        
        response = session.get(url, headers=headers, timeout=10, allow_redirects=True)
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = None
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.text.strip()
        
        # æå–ä½œè€…
        author = None
        
        # æ–¹æ³•1ï¼šç™¾å®¶å·çš„ä½œè€…ä¿¡æ¯ï¼ˆæ–°ç‰ˆé¡µé¢ï¼‰
        if 'baijiahao.baidu.com' in url:
            # æ–°ç‰ˆé¡µé¢ï¼šä»HTMLæ ‡ç­¾æå–
            # <span data-testid="author-name" class="_2gGWi">ä½œè€…å</span>
            author_elem = soup.find('span', {'data-testid': 'author-name'})
            if author_elem:
                author = author_elem.get_text().strip()
            
            # å¤‡ç”¨æ–¹æ³•1ï¼šä»class="_2gGWi"æå–
            if not author:
                author_elem = soup.find('span', class_='_2gGWi')
                if author_elem:
                    author = author_elem.get_text().strip()
            
            # å¤‡ç”¨æ–¹æ³•2ï¼šä»authoré“¾æ¥æå–
            if not author:
                author_link = soup.find('a', href=re.compile('author.baidu.com'))
                if author_link:
                    author_span = author_link.find('span')
                    if author_span:
                        author = author_span.get_text().strip()
            
            # å¤‡ç”¨æ–¹æ³•3ï¼šæ—§ç‰ˆJSONæ•°æ®ï¼ˆå…¼å®¹æ—§é“¾æ¥ï¼‰
            if not author:
                json_match = re.search(r'var\s+DATA\s*=\s*({.+?});', response.text, re.DOTALL)
                if json_match:
                    try:
                        data = json.loads(json_match.group(1))
                        if 'superlanding' in data and len(data['superlanding']) > 0:
                            item = data['superlanding'][0].get('itemData', {})
                            if not title or title == 'ç™¾åº¦':
                                title = item.get('header', title)
                            author = item.get('author', {}).get('name', '')
                    except:
                        pass
        
        # æ–¹æ³•2ï¼šmbd.baiduçš„JSONæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰
        elif 'mbd.baidu.com' in url:
            # æ–¹æ³•2.1: æ ‡å‡†author.nameæ ¼å¼
            author_match = re.search(r'"author"\s*:\s*{\s*[^}]*"name"\s*:\s*"([^"]+)"', response.text)
            if author_match:
                author = author_match.group(1)
                # Unicodeè§£ç 
                try:
                    if '\\u' in author:
                        author = author.encode().decode('unicode_escape')
                except:
                    pass
            
            # æ–¹æ³•2.2: æ›´å¤šä½œè€…å­—æ®µå°è¯•
            if not author:
                author_patterns = [
                    r'"authorName"\s*:\s*"([^"]{2,30})"',
                    r'"author_name"\s*:\s*"([^"]{2,30})"',
                    r'"publisher"\s*:\s*"([^"]{2,30})"',
                    r'"source"\s*:\s*"([^"]{2,30})"',
                    r'"sourceName"\s*:\s*"([^"]{2,30})"',
                    r'"account"\s*:\s*{\s*[^}]*"name"\s*:\s*"([^"]+)"',  # account.nameæ ¼å¼
                ]
                for pattern in author_patterns:
                    match = re.search(pattern, response.text)
                    if match:
                        candidate = match.group(1)
                        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯ä½œè€…çš„
                        if candidate not in ['ç™¾åº¦', 'ç™¾å®¶å·', 'ç™¾åº¦APP', 'ç™¾åº¦æ–°é—»'] and len(candidate) > 1:
                            author = candidate
                            # Unicodeè§£ç 
                            try:
                                if '\\u' in author:
                                    author = author.encode().decode('unicode_escape')
                            except:
                                pass
                            break
            
            # æŸ¥æ‰¾æ ‡é¢˜
            if not title or title == 'ç™¾åº¦':
                title_match = re.search(r'"title"\s*:\s*"([^"]{5,100})"', response.text)
                if title_match:
                    title = title_match.group(1)
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial (ç™¾åº¦ç³»)'
        }
        
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æœªæ‰¾åˆ°ä½œè€…',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_dripcar_info(url):
    """æ°´æ»´æ±½è½¦å¹³å°æå–"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        }
        
        response = session.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = None
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.text.strip()
            # æ¸…ç†æ ‡é¢˜
            if '-æ°´æ»´æ±½è½¦' in title:
                title = title.split('-æ°´æ»´æ±½è½¦')[0].strip()
        
        # æå–ä½œè€… - ä»JavaScriptå˜é‡ä¸­
        author = None
        author_match = re.search(r'author_name\s*:\s*"([^"]+)"', response.text)
        if author_match:
            author = author_match.group(1)
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial'
        }
        
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æœªæ‰¾åˆ°ä½œè€…',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_maiche_info(url):
    """ä¹°è½¦ç½‘å¹³å°æå– - å¢å¼ºç‰ˆ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        }
        
        response = session.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = None
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.text.strip()
            # æ¸…ç†æ ‡é¢˜
            if '- ä¹°è½¦ç½‘' in title:
                title = title.split('- ä¹°è½¦ç½‘')[0].strip()
        
        # æˆ–è€…ä»h1è·å–
        if not title:
            h1 = soup.find('h1')
            if h1:
                title = h1.text.strip()
        
        # æå–ä½œè€… - å¢å¼ºç‰ˆ
        author = None
        
        # æ–¹æ³•1ï¼šä»"æ¥æºï¼š"æˆ–"æ¥æºï¼š"åæå–ï¼ˆä¹°è½¦ç½‘å¸¸ç”¨æ ¼å¼ï¼‰
        # ä¾‹å¦‚ï¼šæ¥æºï¼šå¥¶çˆ¸æ•™é€‰è½¦
        source_patterns = [
            r'æ¥æº[ï¼š:]\s*([^\s\n<>]{2,20})',
            r'source[ï¼š:]\s*([^\s\n<>]{2,20})',
        ]
        for pattern in source_patterns:
            match = re.search(pattern, response.text)
            if match:
                potential_author = match.group(1).strip()
                # è¿‡æ»¤æ‰ä¸€äº›æ˜æ˜¾ä¸æ˜¯ä½œè€…çš„è¯
                if potential_author not in ['ä¹°è½¦ç½‘', 'ç½‘ç»œ', 'äº’è”ç½‘', 'å®˜æ–¹']:
                    author = potential_author
                    break
        
        # æ–¹æ³•2ï¼šä»[è½¦å‹å¤´æ¡-è½¦å‹å·-ä½œè€…å]æ ¼å¼æå–
        if not author:
            match = re.search(r'\[è½¦å‹å¤´æ¡[^\]]*è½¦å‹å·[^\]]*[-\-]\s*([^\]]{2,20})\]', response.text)
            if match:
                author = match.group(1).strip()
        
        # æ–¹æ³•3ï¼šæŸ¥æ‰¾ä½œè€…class
        if not author:
            author_elem = soup.find(class_=re.compile('author', re.I))
            if author_elem:
                author = author_elem.text.strip()
        
        # æ–¹æ³•4ï¼šæŸ¥æ‰¾metaæ ‡ç­¾
        if not author:
            meta_author = soup.find('meta', {'name': 'author'})
            if meta_author:
                author = meta_author.get('content', '').strip()
        
        # æ–¹æ³•5ï¼šä»JSONæ•°æ®ä¸­æŸ¥æ‰¾
        if not author:
            author_match = re.search(r'"author"\s*:\s*"([^"]+)"', response.text)
            if author_match:
                author = author_match.group(1)
        
        # æ–¹æ³•6ï¼šä»"æ–‡/"æˆ–"æ–‡:"åæå–
        if not author:
            match = re.search(r'[ï¼ˆ(]æ–‡[/ï¼:]([^ï¼‰)]{2,20})[ï¼‰)]', response.text)
            if match:
                author = match.group(1).strip()
        
        # ç»Ÿä¸€æ¸…ç†ä½œè€…åç§°ï¼ˆåº”ç”¨äºæ‰€æœ‰æ–¹æ³•ï¼‰
        if author:
            # å»æ‰"è½¦å‹å·"ã€"ä½œè€…ï¼š"ç­‰å‰ç¼€ï¼ˆåŒ…æ‹¬ç©ºæ ¼ï¼‰
            author = re.sub(r'^(è½¦å‹å·|ä½œè€…|æ¥æº)\s*[ï¼š:\s]*', '', author).strip()
            # å†æ¬¡æ¸…ç†å¤šä½™ç©ºæ ¼
            author = ' '.join(author.split())
        
        return {
            'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
            'status': 'success' if (title and author) else 'partial'
        }
        
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æœªæ‰¾åˆ°ä½œè€…',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_dongchedi_info(url):
    """æ‡‚è½¦å¸å¹³å°æå– - Playwrightç‰ˆï¼ˆæ‡‚è½¦å¸éœ€JSæ¸²æŸ“ï¼‰+ requestsé™çº§ + è°ƒè¯•æ—¥å¿—"""
    # æ‡‚è½¦å¸é¡µé¢æ˜¯JSæ¸²æŸ“çš„ï¼Œç›´æ¥ä½¿ç”¨Playwright
    if not PLAYWRIGHT_AVAILABLE:
        return {
            'title': 'Playwrightæœªå®‰è£…',
            'author': 'æœªæ‰¾åˆ°',
            'status': 'failed: æ‡‚è½¦å¸éœ€è¦Playwright'
        }
    
    dcd_debug_log(f"[DEBUG-DCD] å¼€å§‹æå–æ‡‚è½¦å¸: {url[:80]}...")
    playwright_result = None
    
    try:
        with sync_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            browser = None
            try:
                browser = p.chromium.launch(
                    headless=True,
                    args=[
                        '--disable-blink-features=AutomationControlled',
                        '--disable-dev-shm-usage',
                        '--no-sandbox'
                    ]
                )
            except:
                return {
                    'title': 'æµè§ˆå™¨å¯åŠ¨å¤±è´¥',
                    'author': 'æœªæ‰¾åˆ°',
                    'status': 'failed: Browser launch error'
                }
            
            context = browser.new_context(
                user_agent='Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15',
                viewport={'width': 375, 'height': 812},
            )
            page = context.new_page()
            
            # å»é™¤ webdriver æ ‡è®°
            page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)
            
            # è®¿é—®é¡µé¢
            try:
                page.goto(url, wait_until='networkidle', timeout=30000)
                time.sleep(5)  # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿JSå®Œå…¨æ¸²æŸ“
            except:
                try:
                    page.goto(url, wait_until='domcontentloaded', timeout=20000)
                    time.sleep(5)  # å¢åŠ ç­‰å¾…æ—¶é—´
                except:
                    time.sleep(3)
            
            # é¢å¤–ç­‰å¾…ç¡®ä¿ä½œè€…ä¿¡æ¯åŠ è½½å®Œæˆ
            try:
                # ç­‰å¾…å¯èƒ½åŒ…å«ä½œè€…ä¿¡æ¯çš„å…ƒç´ 
                page.wait_for_timeout(2000)  # å†ç­‰2ç§’
            except:
                pass
            
            # è·å–é¡µé¢å†…å®¹
            html_content = page.content()
            browser.close()
            
            dcd_debug_log(f"[DEBUG-DCD] HTMLå†…å®¹é•¿åº¦: {len(html_content)} å­—ç¬¦")
            
            # ä¿å­˜HTMLå†…å®¹åˆ°æ–‡ä»¶ç”¨äºè°ƒè¯•ï¼ˆä»…ä¿å­˜ç¬¬ä¸€ä¸ªï¼‰
            try:
                import os
                debug_file = 'dongchedi_page.html'
                if not os.path.exists(debug_file):
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    dcd_debug_log(f"[DEBUG-DCD] HTMLå·²ä¿å­˜åˆ°: {debug_file}")
            except Exception as e:
                dcd_debug_log(f"[DEBUG-DCD] ä¿å­˜HTMLå¤±è´¥: {str(e)}")
            
            if not html_content or len(html_content) < 500:
                dcd_debug_log(f"[DEBUG-DCD] é”™è¯¯: é¡µé¢å†…å®¹å¤ªçŸ­")
                return {
                    'title': 'é¡µé¢åŠ è½½å¤±è´¥',
                    'author': 'æœªæ‰¾åˆ°',
                    'status': 'failed: é¡µé¢å†…å®¹ä¸ºç©º'
                }
            
            # è§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            title = None
            author = None
            
            # æå–æ ‡é¢˜
            # æ–¹æ³•1: h1æ ‡ç­¾
            h1_elem = soup.find('h1')
            if h1_elem:
                title = h1_elem.get_text().strip()
            
            # æ–¹æ³•2: titleæ ‡ç­¾
            if not title or len(title) < 5:
                title_elem = soup.find('title')
                if title_elem:
                    title = title_elem.get_text().strip()
                    # æ¸…ç†æ ‡é¢˜ï¼ˆå¤„ç†å„ç§åˆ†éš”ç¬¦ï¼‰
                    for suffix in [' - æ‡‚è½¦å¸', '-æ‡‚è½¦å¸', '_æ‡‚è½¦å¸', '|æ‡‚è½¦å¸', ' æ‡‚è½¦å¸']:
                        if suffix in title:
                            title = title.split(suffix)[0].strip()
                            break
            
            # æ–¹æ³•3: metaæ ‡ç­¾
            if not title or len(title) < 5:
                meta_title = soup.find('meta', {'property': 'og:title'})
                if meta_title:
                    title = meta_title.get('content', '').strip()
            
            # æå–ä½œè€…
            # æ–¹æ³•1: ä»JSONæ•°æ®æå–ï¼ˆæ‰©å±•æ›´å¤šæ¨¡å¼ï¼‰
            dcd_debug_log(f"[DEBUG-DCD] å¼€å§‹æå–ä½œè€…ï¼ˆæ–¹æ³•1: JSONæ•°æ®ï¼‰")
            json_patterns = [
                r'"author"\s*:\s*{\s*[^}]*"name"\s*:\s*"([^"]+)"',
                r'"author_name"\s*:\s*"([^"]{2,30})"',
                r'"user_name"\s*:\s*"([^"]{2,30})"',
                r'"userName"\s*:\s*"([^"]{2,30})"',
                r'"nickname"\s*:\s*"([^"]{2,30})"',
                r'"screen_name"\s*:\s*"([^"]{2,30})"',
                r'"source"\s*:\s*"([^"]{2,30})"',
                r'"media_name"\s*:\s*"([^"]{2,30})"',
                r'"name"\s*:\s*"([^"]{2,30})"',  # é€šç”¨nameå­—æ®µ
                r'"creator"\s*:\s*"([^"]{2,30})"',  # åˆ›å»ºè€…
                r'"publisher"\s*:\s*"([^"]{2,30})"',  # å‘å¸ƒè€…
            ]
            
            for i, pattern in enumerate(json_patterns):
                matches = re.findall(pattern, html_content)
                if matches:
                    dcd_debug_log(f"[DEBUG-DCD] æ¨¡å¼{i+1}æ‰¾åˆ°{len(matches)}ä¸ªåŒ¹é…: {matches[:3]}")
                    # å°è¯•æ‰€æœ‰åŒ¹é…ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ä½œè€…
                    for potential_author in matches:
                        dcd_debug_log(f"[DEBUG-DCD] æ£€æŸ¥å€™é€‰ä½œè€…: '{potential_author}'")
                        if (potential_author not in ['æ‡‚è½¦å¸', 'dongchedi', 'ä»Šæ—¥å¤´æ¡', 'toutiao', 'bytedance', 
                                                     'æŠ–éŸ³', 'douyin', 'å­—èŠ‚è·³åŠ¨', 'å¤´æ¡', 'article'] and 
                            len(potential_author) > 1 and
                            not potential_author.startswith('http') and
                            not potential_author.isdigit()):  # æ’é™¤çº¯æ•°å­—
                            author = potential_author
                            dcd_debug_log(f"[DEBUG-DCD] âœ“ æ‰¾åˆ°æœ‰æ•ˆä½œè€…: '{author}'")
                            try:
                                if '\\u' in author:
                                    author = author.encode().decode('unicode_escape')
                            except:
                                pass
                            break
                        else:
                            dcd_debug_log(f"[DEBUG-DCD] âœ— å€™é€‰ä½œè€…è¢«è¿‡æ»¤")
                    if author:
                        break
            
            # æ–¹æ³•2: ä»HTMLå…ƒç´ æå–
            if not author:
                dcd_debug_log(f"[DEBUG-DCD] æ–¹æ³•1å¤±è´¥ï¼Œå°è¯•æ–¹æ³•2: HTMLå…ƒç´ æå–")
                author_selectors = [
                    {'class': re.compile('author|writer|creator', re.I)},
                    {'class': re.compile('source|publisher', re.I)},
                    {'class': re.compile('user|username', re.I)},
                ]
                for i, selector in enumerate(author_selectors):
                    elem = soup.find('span', selector) or soup.find('a', selector) or soup.find('div', selector)
                    if elem:
                        text = elem.get_text().strip()
                        dcd_debug_log(f"[DEBUG-DCD] HTMLé€‰æ‹©å™¨{i+1}æ‰¾åˆ°å…ƒç´ : '{text[:50]}'")
                        if text and 2 <= len(text) <= 30 and text not in ['æ‡‚è½¦å¸', '']:
                            author = text
                            dcd_debug_log(f"[DEBUG-DCD] âœ“ HTMLå…ƒç´ æå–æˆåŠŸ: '{author}'")
                            break
            else:
                dcd_debug_log(f"[DEBUG-DCD] æ–¹æ³•1æˆåŠŸæå–ä½œè€…")
            
            # æ–¹æ³•3: metaæ ‡ç­¾
            if not author:
                meta_author = soup.find('meta', {'name': 'author'}) or soup.find('meta', {'property': 'article:author'})
                if meta_author:
                    potential_author = meta_author.get('content', '').strip()
                    if potential_author not in ['æ‡‚è½¦å¸', '']:
                        author = potential_author
            
            playwright_result = {
                'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
                'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
                'status': 'success (Playwright)' if (title and author) else 'partial (Playwright)'
            }
            
            dcd_debug_log(f"[DEBUG-DCD] æå–ç»“æœ:")
            dcd_debug_log(f"[DEBUG-DCD]   æ ‡é¢˜: {playwright_result['title'][:50]}")
            dcd_debug_log(f"[DEBUG-DCD]   ä½œè€…: {playwright_result['author']}")
            dcd_debug_log(f"[DEBUG-DCD]   çŠ¶æ€: {playwright_result['status']}")
            
            # å¦‚æœPlaywrightæˆåŠŸè·å–äº†ä½œè€…ï¼Œç›´æ¥è¿”å›
            if author:
                return playwright_result
            else:
                dcd_debug_log(f"[DEBUG-DCD] Playwrightæœªèƒ½è·å–ä½œè€…ï¼Œå‡†å¤‡é™çº§åˆ°requests")
            
    except Exception as e:
        playwright_result = {
            'title': 'æå–å¤±è´¥',
            'author': 'æœªæ‰¾åˆ°',
            'status': f'failed: {str(e)[:50]}'
        }
    
    # å¦‚æœPlaywrightæœªèƒ½è·å–ä½œè€…ï¼Œå°è¯•requestsé™çº§æ–¹æ¡ˆ
    if playwright_result and 'æœªæ‰¾åˆ°' in playwright_result.get('author', ''):
        try:
            print("  [é™çº§åˆ°requestså°è¯•]", end='', flush=True)
            headers = {
                'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            }
            response = session.get(url, headers=headers, timeout=15)
            response.encoding = 'utf-8'
            
            author = None
            # ä»requestså“åº”ä¸­æå–ä½œè€…
            json_patterns = [
                r'"author_name"\s*:\s*"([^"]{2,30})"',
                r'"user_name"\s*:\s*"([^"]{2,30})"',
                r'"userName"\s*:\s*"([^"]{2,30})"',
                r'"nickname"\s*:\s*"([^"]{2,30})"',
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, response.text)
                if matches:
                    for potential_author in matches:
                        if (potential_author not in ['æ‡‚è½¦å¸', 'dongchedi', 'ä»Šæ—¥å¤´æ¡'] and 
                            len(potential_author) > 1):
                            author = potential_author
                            break
                    if author:
                        break
            
            if author:
                playwright_result['author'] = author
                playwright_result['status'] = 'success (requestsé™çº§)'
                print("âœ“", flush=True)
            
        except:
            pass
    
    return playwright_result

def extract_toutiao_playwright(url):
    """ä»Šæ—¥å¤´æ¡ä¸“ç”¨ - Playwrightå¢å¼ºç‰ˆ"""
    if not PLAYWRIGHT_AVAILABLE:
        # é™çº§åˆ°requests
        return extract_general_info(url)
    
    try:
        with sync_playwright() as p:
            # å°è¯•å¤šç§æ–¹å¼å¯åŠ¨æµè§ˆå™¨
            browser = None
            launch_errors = []
            
            # æ–¹å¼1: é»˜è®¤å¯åŠ¨
            if not browser:
                try:
                    browser = p.chromium.launch(
                        headless=True,
                        args=[
                            '--disable-blink-features=AutomationControlled',
                            '--disable-dev-shm-usage',
                            '--no-sandbox'
                        ]
                    )
                except Exception as e:
                    launch_errors.append(f"é»˜è®¤å¯åŠ¨å¤±è´¥: {str(e)[:50]}")
            
            # æ–¹å¼2: ä¸æŒ‡å®šexecutable_path
            if not browser:
                try:
                    browser = p.chromium.launch(
                        headless=True,
                        executable_path=None,
                        args=[
                            '--disable-blink-features=AutomationControlled',
                            '--disable-dev-shm-usage',
                            '--no-sandbox'
                        ]
                    )
                except Exception as e:
                    launch_errors.append(f"æ— è·¯å¾„å¯åŠ¨å¤±è´¥: {str(e)[:50]}")
            
            # æ–¹å¼3: å°è¯•æŒ‡å®šç³»ç»Ÿè·¯å¾„
            if not browser:
                import os
                possible_paths = [
                    os.path.expanduser("~/.cache/ms-playwright/chromium-*/chrome-win/chrome.exe"),
                    os.path.expanduser("~/AppData/Local/ms-playwright/chromium-*/chrome-win/chrome.exe"),
                    "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe",
                    "C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe"
                ]
                for path in possible_paths:
                    try:
                        if '*' in path:
                            import glob
                            matches = glob.glob(path)
                            if matches:
                                path = matches[0]
                        if os.path.exists(path):
                            browser = p.chromium.launch(
                                headless=True,
                                executable_path=path,
                                args=[
                                    '--disable-blink-features=AutomationControlled',
                                    '--disable-dev-shm-usage',
                                    '--no-sandbox'
                                ]
                            )
                            break
                    except Exception as e:
                        launch_errors.append(f"è·¯å¾„{path}å¯åŠ¨å¤±è´¥: {str(e)[:30]}")
                    
            # å¦‚æœæ‰€æœ‰æ–¹å¼éƒ½å¤±è´¥
            if not browser:
                error_msg = "; ".join(launch_errors) if launch_errors else "æœªçŸ¥é”™è¯¯"
                return {
                    'title': 'æµè§ˆå™¨å¯åŠ¨å¤±è´¥',
                    'author': 'æœªæ‰¾åˆ°',
                    'status': f'failed: Browser launch error - {error_msg}'
                }
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                extra_http_headers={
                    'Accept-Language': 'zh-CN,zh;q=0.9',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                }
            )
            page = context.new_page()
            
            # å¢å¼ºåæ£€æµ‹
            page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5]
                });
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['zh-CN', 'zh', 'en']
                });
            """)
            
            # è®¿é—®é¡µé¢ - å¢åŠ ç­‰å¾…æ—¶é—´
            try:
                page.goto(url, wait_until='networkidle', timeout=45000)
                time.sleep(5)  # å¢åŠ ç­‰å¾…æ—¶é—´
            except Exception as e:
                print(f"é¡µé¢åŠ è½½è­¦å‘Š: {str(e)[:50]}")
                try:
                    # å¦‚æœnetworkidleè¶…æ—¶ï¼Œå°è¯•domcontentloaded
                    page.goto(url, wait_until='domcontentloaded', timeout=30000)
                    time.sleep(5)
                except:
                    pass
            
            # å°è¯•å¤šæ¬¡è·å–å†…å®¹
            html_content = None
            for attempt in range(3):
                try:
                    html_content = page.content()
                    if html_content and len(html_content) > 1000:
                        break
                    time.sleep(2)
                except:
                    if attempt < 2:
                        time.sleep(2)
            
            browser.close()
            
            if not html_content or len(html_content) < 500:
                return {
                    'title': 'é¡µé¢åŠ è½½å¤±è´¥',
                    'author': 'æœªæ‰¾åˆ°ä½œè€…',
                    'status': 'failed: é¡µé¢å†…å®¹ä¸ºç©º'
                }
            
            # è§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            title = None
            author = None
            
            # æå–æ ‡é¢˜ - å¤šç§æ–¹æ³•
            # æ–¹æ³•1: metaæ ‡ç­¾ï¼ˆæœ€å‡†ç¡®ï¼‰
            meta_title = soup.find('meta', {'property': 'og:title'})
            if meta_title and meta_title.get('content'):
                title = meta_title.get('content', '').strip()
            
            # æ–¹æ³•2: h1æ ‡ç­¾
            if not title or len(title) < 5:
                h1_elem = soup.find('h1')
                if h1_elem:
                    h1_text = h1_elem.get_text().strip()
                    if len(h1_text) >= 5:  # ç¡®ä¿h1å†…å®¹è¶³å¤Ÿé•¿
                        title = h1_text
            
            # æ–¹æ³•3: titleæ ‡ç­¾
            if not title or len(title) < 5:
                title_elem = soup.find('title')
                if title_elem:
                    title_text = title_elem.get_text().strip()
                    # æ¸…ç†æ ‡é¢˜ï¼ˆå»æ‰ç½‘ç«™åï¼‰
                    if '_' in title_text:
                        title = title_text.split('_')[0].strip()
                    elif '-' in title_text:
                        title = title_text.split('-')[0].strip()
                    else:
                        title = title_text
            
            # æ–¹æ³•4: ä»JSONä¸­æå–titleå­—æ®µ
            if not title or len(title) < 5:
                title_match = re.search(r'"title"\s*:\s*"([^"]{10,200})"', html_content)
                if title_match:
                    title = title_match.group(1)
            
            # æå–ä½œè€… - å¢å¼ºå¤šç§æ–¹æ³•
            # æ–¹æ³•1: ä»JSONæ•°æ®ä¸­æå–ï¼ˆæ‰©å±•æ¨¡å¼ï¼‰
            json_patterns = [
                r'"name"\s*:\s*"([^"]{2,30})"',  # ä½œè€…å
                r'"nickname"\s*:\s*"([^"]{2,30})"',  # æ˜µç§°
                r'"screen_name"\s*:\s*"([^"]{2,30})"',  # å±å¹•å
                r'"userName"\s*:\s*"([^"]{2,30})"',  # ç”¨æˆ·å
                r'"source"\s*:\s*"([^"]{2,30})"',  # æ¥æº
                r'"author_name"\s*:\s*"([^"]{2,30})"',
                r'"user_name"\s*:\s*"([^"]{2,30})"',
                r'\"ä½œè€…\"\s*:\s*\"([^\"]{2,30})\"',
                r'"media_name"\s*:\s*"([^"]{2,30})"',  # åª’ä½“å
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, html_content)
                if matches:
                    # è¿‡æ»¤æ‰å¯èƒ½çš„è¯¯åŒ¹é…
                    for match in matches:
                        if match and len(match) >= 2 and len(match) <= 30:
                            # æ’é™¤å¸¸è§è¯¯åŒ¹é…
                            exclude_words = ['ä»Šæ—¥å¤´æ¡', 'toutiao', 'article', 'content', 'title', 
                                           'video', 'image', 'button', 'input', 'label']
                            if not any(word in match.lower() for word in exclude_words):
                                # æ¸…ç†æ—¥æœŸï¼šç§»é™¤ç±»ä¼¼"2025-10-17 15:29"çš„æ—¥æœŸæ—¶é—´
                                cleaned_author = re.sub(r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}.*$', '', match).strip()
                                if cleaned_author and len(cleaned_author) >= 2:
                                    author = cleaned_author
                                    break
                    if author:
                        break
            
            # æ–¹æ³•2: ä»HTMLå…ƒç´ æå–ï¼ˆæ‰©å±•é€‰æ‹©å™¨ï¼‰
            if not author:
                author_selectors = [
                    {'class': re.compile('author|writer|creator', re.I)},
                    {'class': re.compile('source|publisher', re.I)},
                    {'class': re.compile('user|username', re.I)},
                    {'data-id': True},
                    {'id': re.compile('author', re.I)},
                ]
                for selector in author_selectors:
                    elem = soup.find('span', selector) or soup.find('a', selector) or soup.find('div', selector)
                    if elem:
                        text = elem.get_text().strip()
                        # æ¸…ç†æ—¥æœŸæ—¶é—´
                        text = re.sub(r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}.*$', '', text).strip()
                        if text and 2 <= len(text) <= 30:
                            author = text
                            break
            
            # æ–¹æ³•3: ä»metaæ ‡ç­¾æå–
            if not author:
                meta_author = soup.find('meta', {'name': 'author'}) or soup.find('meta', {'property': 'article:author'})
                if meta_author:
                    author = meta_author.get('content', '').strip()
            
            # æ–¹æ³•4: ä»URLä¸­æå–ç”¨æˆ·ä¿¡æ¯ï¼ˆéƒ¨åˆ†ä»Šæ—¥å¤´æ¡é“¾æ¥åŒ…å«ç”¨æˆ·ä¿¡æ¯ï¼‰
            if not author:
                # å°è¯•ä»share_uidæˆ–å…¶ä»–å‚æ•°æå–
                uid_match = re.search(r'share_uid=([^&]+)', url)
                if uid_match and 'AAAA' not in uid_match.group(1):
                    # è¿™åªæ˜¯å¤‡ç”¨æ–¹æ¡ˆï¼Œé€šå¸¸ä¸ä¼šç”¨åˆ°
                    pass
            
            # æœ€ç»ˆæ¸…ç†ä½œè€…å­—æ®µï¼ˆç§»é™¤æ—¥æœŸï¼‰
            if author:
                author = re.sub(r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}.*$', '', author).strip()
            
            return {
                'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
                'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
                'status': 'success' if (title and author) else 'partial'
            }
            
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def is_baidu_or_douyin(url):
    """æ£€æŸ¥æ˜¯å¦æ˜¯ç™¾åº¦ç³»ã€æŠ–éŸ³æˆ–æ‡‚è½¦å¸é“¾æ¥ï¼ˆéœ€è¦Playwrightï¼‰"""
    url_lower = url.lower()
    return ('baijiahao.baidu.com' in url_lower or 
            'mbd.baidu.com' in url_lower or 
            'douyin.com' in url_lower or 
            'iesdouyin.com' in url_lower or
            'zjbyte.cn' in url_lower or
            'dongchedi' in url_lower or
            'dcd.' in url_lower)

def extract_platform_info(url):
    """è¯†åˆ«å¹³å°å¹¶ä½¿ç”¨ç‰¹å®šæ–¹æ³•æå–ä¿¡æ¯"""
    url_lower = url.lower()
    
    if 'mp.weixin.qq' in url_lower:
        return extract_weixin_info(url)
    elif 'weibo.com' in url_lower:
        return extract_weibo_breakthrough(url)
    elif 'toutiao.com' in url_lower:
        # ä»Šæ—¥å¤´æ¡ä½¿ç”¨Playwright
        return extract_toutiao_playwright(url)
    elif 'douyin.com' in url_lower or 'iesdouyin.com' in url_lower:
        return extract_douyin_enhanced(url)
    elif 'xiaohongshu.com' in url_lower or 'xhslink.com' in url_lower:
        return extract_xiaohongshu_info(url)
    elif 'bilibili.com' in url_lower:
        return extract_bilibili_info(url)
    elif 'autohome.com' in url_lower:
        return extract_autohome_info(url)
    elif 'zjbyte.cn' in url_lower or 'dongchedi' in url_lower or 'dcd.' in url_lower:
        return extract_dongchedi_info(url)
    elif 'baijiahao.baidu.com' in url_lower or 'mbd.baidu.com' in url_lower:
        return extract_baidu_info(url)
    elif 'dripcar.com' in url_lower:
        return extract_dripcar_info(url)
    elif 'maiche.com' in url_lower:
        return extract_maiche_info(url)
    else:
        return extract_general_info(url)

def extract_with_playwright_browser(url):
    """ä½¿ç”¨ Playwright æµè§ˆå™¨æ–¹æ¡ˆå¤„ç†éš¾å¤„ç†çš„å¹³å°ï¼ˆç™¾åº¦ç³»ã€æŠ–éŸ³ï¼‰"""
    if not PLAYWRIGHT_AVAILABLE:
        return {
            'title': 'Playwrightæœªå®‰è£…',
            'author': 'æœªæ‰¾åˆ°',
            'status': 'failed: Playwrightæœªå®‰è£…'
        }
    
    try:
        with sync_playwright() as p:
            # å°è¯•å¤šç§æ–¹å¼å¯åŠ¨æµè§ˆå™¨
            browser = None
            launch_errors = []
            
            # æ–¹å¼1: é»˜è®¤å¯åŠ¨
            if not browser:
                try:
                    browser = p.chromium.launch(
                        headless=True,
                        args=[
                            '--disable-blink-features=AutomationControlled',
                            '--disable-dev-shm-usage',
                            '--no-sandbox'
                        ]
                    )
                except Exception as e:
                    launch_errors.append(f"é»˜è®¤å¯åŠ¨å¤±è´¥: {str(e)[:50]}")
            
            # æ–¹å¼2: ä¸æŒ‡å®šexecutable_path
            if not browser:
                try:
                    browser = p.chromium.launch(
                        headless=True,
                        executable_path=None,
                        args=[
                            '--disable-blink-features=AutomationControlled',
                            '--disable-dev-shm-usage',
                            '--no-sandbox'
                        ]
                    )
                except Exception as e:
                    launch_errors.append(f"æ— è·¯å¾„å¯åŠ¨å¤±è´¥: {str(e)[:50]}")
            
            # æ–¹å¼3: å°è¯•æŒ‡å®šç³»ç»Ÿè·¯å¾„
            if not browser:
                import os
                possible_paths = [
                    os.path.expanduser("~/.cache/ms-playwright/chromium-*/chrome-win/chrome.exe"),
                    os.path.expanduser("~/AppData/Local/ms-playwright/chromium-*/chrome-win/chrome.exe"),
                    "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe",
                    "C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe"
                ]
                for path in possible_paths:
                    try:
                        if '*' in path:
                            import glob
                            matches = glob.glob(path)
                            if matches:
                                path = matches[0]
                        if os.path.exists(path):
                            browser = p.chromium.launch(
                                headless=True,
                                executable_path=path,
                                args=[
                                    '--disable-blink-features=AutomationControlled',
                                    '--disable-dev-shm-usage',
                                    '--no-sandbox'
                                ]
                            )
                            break
                    except Exception as e:
                        launch_errors.append(f"è·¯å¾„{path}å¯åŠ¨å¤±è´¥: {str(e)[:30]}")
                    
            # å¦‚æœæ‰€æœ‰æ–¹å¼éƒ½å¤±è´¥
            if not browser:
                error_msg = "; ".join(launch_errors) if launch_errors else "æœªçŸ¥é”™è¯¯"
                return {
                    'title': 'æµè§ˆå™¨å¯åŠ¨å¤±è´¥',
                    'author': 'æœªæ‰¾åˆ°',
                    'status': f'failed: Browser launch error - {error_msg}'
                }
            # ç™¾åº¦ç³»æ£€æµ‹å¾ˆä¸¥ï¼Œä½¿ç”¨æ‰‹æœºUAæ›´å®¹æ˜“é€šè¿‡
            user_agent = 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1' if 'baidu' in url.lower() else 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            viewport = {'width': 375, 'height': 812} if 'baidu' in url.lower() else {'width': 1920, 'height': 1080}
            
            context = browser.new_context(
                user_agent=user_agent,
                viewport=viewport,
            )
            page = context.new_page()
            
            # å»é™¤ webdriver æ ‡è®°
            page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)
            
            # è®¿é—®é¡µé¢ï¼ˆä½¿ç”¨domcontentloadedæ›´å¿«ä¸”æˆåŠŸç‡æ›´é«˜ï¼‰
            try:
                page.goto(url, wait_until='domcontentloaded', timeout=20000)
                time.sleep(3)  # ç­‰å¾… JavaScript æ¸²æŸ“
            except:
                # å³ä½¿è¶…æ—¶ä¹Ÿå°è¯•ç»§ç»­
                try:
                    time.sleep(2)
                except:
                    pass
            
            # è·å–é¡µé¢å†…å®¹
            html_content = page.content()
            browser.close()
            
            # è§£æ HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            url_lower = url.lower()
            
            title = None
            author = None
            
            # æå–æ ‡é¢˜
            # æ–¹æ³•1: h1 æ ‡ç­¾
            h1_elem = soup.find('h1')
            if h1_elem:
                title = h1_elem.get_text().strip()
            
            # æ–¹æ³•2: title æ ‡ç­¾
            if not title or len(title) < 5:
                title_elem = soup.find('title')
                if title_elem:
                    title = title_elem.get_text().strip()
                    # æ¸…ç†æ ‡é¢˜
                    for sep in ['_', ' - ', '-']:
                        if sep in title:
                            parts = title.split(sep)
                            if len(parts[0].strip()) > 5:
                                title = parts[0].strip()
                                break
            
            # æ–¹æ³•3: meta æ ‡ç­¾
            if not title or len(title) < 5:
                meta_title = soup.find('meta', {'property': 'og:title'})
                if meta_title:
                    title = meta_title.get('content', '').strip()
            
            # æå–ä½œè€…
            # é’ˆå¯¹ç™¾åº¦ç³»
            if 'baidu' in url_lower:
                # æ–¹æ³•1: JSON æ•°æ®ï¼ˆä¼˜å…ˆï¼‰
                author_patterns = [
                    r'"author"\s*:\s*"([^"]{2,30})"',  # ç®€å•authorå­—æ®µ
                    r'"author"\s*:\s*{\s*[^}]*"name"\s*:\s*"([^"]+)"',  # åµŒå¥—author.name
                    r'"authorName"\s*:\s*"([^"]{2,30})"',
                    r'"author_name"\s*:\s*"([^"]{2,30})"',
                    r'"userName"\s*:\s*"([^"]{2,30})"',
                ]
                for pattern in author_patterns:
                    match = re.search(pattern, html_content)
                    if match:
                        potential_author = match.group(1)
                        # è¿‡æ»¤æ˜æ˜¾ä¸æ˜¯ä½œè€…çš„
                        if potential_author not in ['ç™¾åº¦', 'baidu', 'å¥½çœ‹è§†é¢‘']:
                            author = potential_author
                            break
                
                # æ–¹æ³•2: HTML æ ‡ç­¾
                if not author:
                    author_elem = soup.find('span', {'data-testid': 'author-name'})
                    if author_elem:
                        author = author_elem.get_text().strip()
                
                if not author:
                    author_elem = soup.find('span', class_='_2gGWi')
                    if author_elem:
                        author = author_elem.get_text().strip()
                
                # æ–¹æ³•3: æŸ¥æ‰¾classåŒ…å«authorçš„å…ƒç´ 
                if not author:
                    author_elem = soup.find(class_=re.compile('author', re.I))
                    if author_elem:
                        text = author_elem.get_text().strip()
                        # æå–ä¸­æ–‡ä½œè€…åï¼ˆ2-30ä¸ªå­—ç¬¦ï¼‰
                        match = re.search(r'[\u4e00-\u9fa5a-zA-Z0-9]{2,30}', text)
                        if match:
                            author = match.group(0)
            
            # é’ˆå¯¹æŠ–éŸ³
            elif 'douyin' in url_lower:
                # æ–¹æ³•1: JSON æ•°æ®æ·±åº¦æœç´¢
                render_match = re.search(r'<script id="RENDER_DATA" type="application/json">([^<]+)</script>', html_content)
                if render_match:
                    try:
                        json_str = html_module.unescape(render_match.group(1))
                        data = json.loads(json_str)
                        
                        def deep_find_author(obj, depth=0, max_depth=10):
                            if depth > max_depth:
                                return None
                            
                            if isinstance(obj, dict):
                                author_keys = ['nickname', 'authorName', 'unique_id', 'userName']
                                for key in author_keys:
                                    if key in obj and isinstance(obj[key], str) and 2 < len(obj[key]) < 50:
                                        return obj[key]
                                
                                for key in ['author', 'user']:
                                    if key in obj and isinstance(obj[key], dict):
                                        result = deep_find_author(obj[key], depth+1, max_depth)
                                        if result:
                                            return result
                                
                                for value in obj.values():
                                    result = deep_find_author(value, depth+1, max_depth)
                                    if result:
                                        return result
                            
                            elif isinstance(obj, list):
                                for item in obj:
                                    result = deep_find_author(item, depth+1, max_depth)
                                    if result:
                                        return result
                            
                            return None
                        
                        author = deep_find_author(data)
                    except:
                        pass
                
                # æ–¹æ³•2: æ­£åˆ™è¡¨è¾¾å¼
                if not author:
                    patterns = [
                        r'"nickname":\s*"([^"]{2,30})"',
                        r'"authorName":\s*"([^"]{2,30})"',
                    ]
                    for pattern in patterns:
                        matches = re.findall(pattern, html_content)
                        if matches:
                            author = matches[0]
                            break
            
            # é€šç”¨ä½œè€…æå–ï¼ˆfallbackï¼‰
            if not author:
                meta_author = soup.find('meta', {'name': 'author'})
                if meta_author:
                    author = meta_author.get('content', '').strip()
            
            return {
                'title': title if title else 'æœªæ‰¾åˆ°æ ‡é¢˜',
                'author': author if author else 'æœªæ‰¾åˆ°ä½œè€…',
                'status': 'success (Playwright)' if (title and author) else 'partial (Playwright)'
            }
            
    except Exception as e:
        return {
            'title': 'æå–å¤±è´¥',
            'author': 'æå–å¤±è´¥',
            'status': f'failed: {str(e)[:50]}'
        }

def extract_title_and_author(url):
    """ä»URLæå–æ ‡é¢˜å’Œä½œè€…ä¿¡æ¯"""
    return extract_platform_info(url)

def main():
    print("=" * 60)
    print("é“¾æ¥æ ‡é¢˜å’Œä½œè€…æå–å·¥å…· v4.6 - ä¸¤é˜¶æ®µå¤„ç†ç‰ˆ")
    print("âœ… é˜¶æ®µ1: å…ˆå¤„ç†æ™®é€šé“¾æ¥ï¼ˆè·³è¿‡éœ€Playwrightçš„å¹³å°ï¼‰")
    print("âœ… é˜¶æ®µ2: ç”¨Playwrightæ‰¹é‡å¤„ç†ç™¾åº¦/æŠ–éŸ³/æ‡‚è½¦å¸")
    print("ğŸ”´ 404é”™è¯¯æ•´è¡Œæ ‡çº¢ | ğŸŸ¡ å¤±è´¥å•å…ƒæ ¼æ ‡é»„")
    print("=" * 60)
    
    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–æ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
    import sys
    if len(sys.argv) > 1:
        excel_file = sys.argv[1]
    else:
        excel_file = 'æµ‹è¯•æ•°æ®é›†_éšæœºæŠ½æ ·.xlsx'
    
    max_links = None  # å¤„ç†å…¨éƒ¨æ•°æ®
    
    print(f"\næ­£åœ¨è¯»å–æ–‡ä»¶: {excel_file}")
    links = read_excel_with_links(excel_file)
    
    print(f"æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
    
    # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆå¯é€‰ï¼‰
    if max_links and len(links) > max_links:
        links = links[:max_links]
        print(f"âš ï¸  é™åˆ¶å¤„ç†å‰ {max_links} æ¡é“¾æ¥\n")
    else:
        print(f"å¤„ç†å…¨éƒ¨ {len(links)} æ¡é“¾æ¥\n")
    
    results = []
    delayed_links = []  # å­˜å‚¨ç™¾åº¦ç³»å’ŒæŠ–éŸ³é“¾æ¥ï¼Œå»¶è¿Ÿå¤„ç†
    start_time = time.time()
    
    # ========== é˜¶æ®µ1: å¤„ç†æ™®é€šé“¾æ¥ï¼Œè·³è¿‡éœ€Playwrightçš„å¹³å° ==========
    print("\n" + "=" * 60)
    print("ã€é˜¶æ®µ1ã€‘å¤„ç†æ™®é€šé“¾æ¥ï¼ˆç™¾åº¦/æŠ–éŸ³/æ‡‚è½¦å¸å°†åœ¨é˜¶æ®µ2å¤„ç†ï¼‰")
    print("=" * 60 + "\n")
    
    for idx, link_info in enumerate(links, 1):
        website_name = get_website_name(link_info['url'])
        url_short = link_info['url'][:60]
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦Playwrightï¼ˆç™¾åº¦/æŠ–éŸ³/æ‡‚è½¦å¸ï¼‰
        if is_baidu_or_douyin(link_info['url']):
            print(f"[{idx}/{len(links)}] {website_name} | {url_short}... â¸ï¸  å»¶è¿Ÿå¤„ç†", flush=True)
            delayed_links.append({
                'idx': idx,
                'link_info': link_info,
                'website_name': website_name
            })
            # æ·»åŠ å ä½ç»“æœ
            results.append({
                'åŸé“¾æ¥': link_info['url'],
                'ç½‘ç«™å': website_name,
                'ä½œè€…': 'å¾…å¤„ç†',
                'æ ‡é¢˜': 'å¾…å¤„ç†',
                'çŠ¶æ€': 'pending'
            })
            continue
        
        # å¤„ç†æ™®é€šé“¾æ¥
        iter_start = time.time()
        print(f"[{idx}/{len(links)}] {website_name} | {url_short}...", end=' ', flush=True)
        
        info = extract_title_and_author(link_info['url'])
        
        iter_time = time.time() - iter_start
        status_icon = 'âœ…' if info['status'] == 'success' else ('âš ï¸' if 'partial' in info['status'] else 'âŒ')
        print(f"{status_icon} ({iter_time:.1f}s)", flush=True)
        
        results.append({
            'åŸé“¾æ¥': link_info['url'],
            'ç½‘ç«™å': website_name,
            'ä½œè€…': info['author'],
            'æ ‡é¢˜': info['title'],
            'çŠ¶æ€': info['status']
        })
        
        # æ¯10æ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ç»Ÿè®¡
        if idx % 10 == 0:
            elapsed = time.time() - start_time
            processed = idx - len(delayed_links)
            if processed > 0:
                avg_time = elapsed / processed
                remaining_normal = (len(links) - idx) * avg_time
                success = sum(1 for r in results if 'success' in r['çŠ¶æ€'])
                print(f"  â±ï¸  å·²ç”¨{elapsed/60:.1f}åˆ† | é¢„è®¡å‰©ä½™{remaining_normal/60:.1f}åˆ† | æˆåŠŸç‡{success/processed*100:.1f}% | å»¶è¿Ÿ{len(delayed_links)}ä¸ª\n")
        
        if idx < len(links):
            time.sleep(0.5)
    
    # ========== é˜¶æ®µ2: ç”¨ Playwright å¤„ç†å»¶è¿Ÿçš„é“¾æ¥ ==========
    if delayed_links:
        print("\n" + "=" * 60)
        print(f"ã€é˜¶æ®µ2ã€‘ä½¿ç”¨Playwrightå¤„ç†å»¶è¿Ÿçš„{len(delayed_links)}ä¸ªé“¾æ¥")
        print("=" * 60 + "\n")
        
        stage2_start = time.time()
        
        for delayed_idx, delayed_item in enumerate(delayed_links, 1):
            idx = delayed_item['idx']
            link_info = delayed_item['link_info']
            website_name = delayed_item['website_name']
            url_short = link_info['url'][:60]
            
            iter_start = time.time()
            print(f"[{delayed_idx}/{len(delayed_links)}] {website_name} | {url_short}...", end=' ', flush=True)
            
            # ä½¿ç”¨å¹³å°ä¸“ç”¨å‡½æ•°å¤„ç†ï¼ˆä¼šè‡ªåŠ¨è·¯ç”±åˆ°å¯¹åº”çš„æå–å‡½æ•°ï¼‰
            info = extract_platform_info(link_info['url'])
            
            iter_time = time.time() - iter_start
            status_icon = 'âœ…' if 'success' in info['status'] else ('âš ï¸' if 'partial' in info['status'] else 'âŒ')
            print(f"{status_icon} ({iter_time:.1f}s)", flush=True)
            
            # æ›´æ–°resultsä¸­å¯¹åº”ä½ç½®çš„ç»“æœ
            results[idx - 1] = {
                'åŸé“¾æ¥': link_info['url'],
                'ç½‘ç«™å': website_name,
                'ä½œè€…': info['author'],
                'æ ‡é¢˜': info['title'],
                'çŠ¶æ€': info['status']
            }
            
            # æ¯5æ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if delayed_idx % 5 == 0 or delayed_idx == len(delayed_links):
                elapsed = time.time() - stage2_start
                avg_time = elapsed / delayed_idx
                remaining = (len(delayed_links) - delayed_idx) * avg_time
                success = sum(1 for i in range(delayed_idx) if 'success' in results[delayed_links[i]['idx'] - 1]['çŠ¶æ€'])
                print(f"  â±ï¸  é˜¶æ®µ2å·²ç”¨{elapsed/60:.1f}åˆ† | é¢„è®¡å‰©ä½™{remaining/60:.1f}åˆ† | æˆåŠŸç‡{success/delayed_idx*100:.1f}%\n")
            
            # Playwright å¤„ç†éœ€è¦æ›´é•¿å»¶è¿Ÿ
            if delayed_idx < len(delayed_links):
                time.sleep(2)
    
    df = pd.DataFrame(results)
    output_file = 'é“¾æ¥åˆ†æç»“æœ_v4_æœ€ç»ˆç‰ˆ.xlsx'
    df.to_excel(output_file, index=False, engine='openpyxl')
    
    # æ·»åŠ é¢œè‰²æ ‡è®°
    print("\næ·»åŠ é¢œè‰²æ ‡è®°...")
    wb = openpyxl.load_workbook(output_file)
    ws = wb.active
    
    # å®šä¹‰é¢œè‰²
    red_fill = PatternFill(start_color='FFCCCC', end_color='FFCCCC', fill_type='solid')  # æµ…çº¢è‰²
    yellow_fill = PatternFill(start_color='FFFF99', end_color='FFFF99', fill_type='solid')  # æµ…é»„è‰²
    
    # éå†æ•°æ®è¡Œï¼ˆä»ç¬¬2è¡Œå¼€å§‹ï¼Œç¬¬1è¡Œæ˜¯è¡¨å¤´ï¼‰
    for idx, row_data in enumerate(results, start=2):
        status = row_data['çŠ¶æ€'].lower()
        title = row_data['æ ‡é¢˜']
        author = row_data['ä½œè€…']
        
        # 404é”™è¯¯ â†’ æ•´è¡Œæ ‡çº¢
        if '404' in status or '404' in title:
            for col in range(1, 6):  # A-Eåˆ—ï¼ˆåŸé“¾æ¥ã€ç½‘ç«™åã€ä½œè€…ã€æ ‡é¢˜ã€çŠ¶æ€ï¼‰
                ws.cell(row=idx, column=col).fill = red_fill
        else:
            # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦æœ‰é—®é¢˜ï¼ˆæœªæ‰¾åˆ°ã€å¤±è´¥ã€é”™è¯¯ç­‰ï¼‰
            title_error_keywords = ['æœªæ‰¾åˆ°', 'æå–å¤±è´¥', 'API', 'å¤±è´¥', 'é”™è¯¯', 'å¾…å¤„ç†', 
                                   'å“åº”', 'è¯·æ±‚', 'URLæ ¼å¼', 'é¡µé¢åŠ è½½', 'æµè§ˆå™¨å¯åŠ¨', 'éªŒè¯ç ä¸­é—´é¡µ']
            if any(keyword in title for keyword in title_error_keywords):
                ws.cell(row=idx, column=4).fill = yellow_fill  # Dåˆ—ï¼šæ ‡é¢˜
            
            # æ£€æŸ¥ä½œè€…æ˜¯å¦æœ‰é—®é¢˜
            author_error_keywords = ['æœªæ‰¾åˆ°', 'æå–å¤±è´¥', 'API', 'å¤±è´¥', 'é”™è¯¯', 'å¾…å¤„ç†',
                                    'å“åº”', 'è¯·æ±‚', 'URLæ ¼å¼', 'é¡µé¢åŠ è½½', 'æµè§ˆå™¨å¯åŠ¨', 'éªŒè¯ç ä¸­é—´é¡µ']
            if any(keyword in author for keyword in author_error_keywords):
                ws.cell(row=idx, column=3).fill = yellow_fill  # Cåˆ—ï¼šä½œè€…
    
    wb.save(output_file)
    
    print("\n" + "=" * 60)
    print(f"åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("ğŸ”´ 404é”™è¯¯ â†’ æ•´è¡Œæ ‡çº¢")
    print("ğŸŸ¡ æ ‡é¢˜/ä½œè€…æå–å¤±è´¥ â†’ å¯¹åº”å•å…ƒæ ¼æ ‡é»„")
    print("=" * 60)
    
    success_count = sum(1 for r in results if 'success' in r['çŠ¶æ€'].lower())
    partial_count = sum(1 for r in results if 'partial' in r['çŠ¶æ€'].lower())
    failed_count = len(results) - success_count - partial_count
    
    print(f"\nã€ç»Ÿè®¡ã€‘")
    print(f"æ€»é“¾æ¥æ•°: {len(results)}")
    print(f"å®Œå…¨æˆåŠŸ: {success_count} ({success_count/len(results)*100:.1f}%)")
    print(f"éƒ¨åˆ†æˆåŠŸ: {partial_count} ({partial_count/len(results)*100:.1f}%)")
    print(f"å¤±è´¥: {failed_count} ({failed_count/len(results)*100:.1f}%)")
    
    print(f"\nã€å¹³å°åˆ†å¸ƒã€‘")
    platform_count = {}
    for r in results:
        platform = r['ç½‘ç«™å']
        platform_count[platform] = platform_count.get(platform, 0) + 1
    
    for platform, count in sorted(platform_count.items(), key=lambda x: x[1], reverse=True):
        print(f"{platform}: {count}ä¸ª")
    
    print("\nå¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()

